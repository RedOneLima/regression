{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{center}\n",
    "\\author{Kyle Hewitt}\n",
    "\\end{center}\n",
    "\\pagenumbering{roman}\n",
    "\\newpage\n",
    "\n",
    "\\section{Introduction}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\pagenumbering{arabic}\n",
    "\n",
    "Regression Machine Learning is a way to create predictive models from exsisting data to predict outcomes of new, never before seen data. This style of machine learning uses exsisting data to train and access the quality of the predicted model by splitting the exsisting data into training, validation and testing sets. As we will see when exploring farther into the different regression models, it is very easy to overfit a model to the training data so that the model looks like its perfect for the information its given but when new data is introducted, the predictions can be wildly wrong and sparatic. The idea of Regression models is to use the training data to create a model and then use the validation and testing sets to verify that these models are not overfit and how well they estimate on data outside the training set. There are different error metrics used throughout to test this idea of how well the model fits such as Residual Sum of Squares and RSME. In our experimentation we will be using the gradiant decent and coodinate decent algorithms in order to find the optimal model parameters, but it is worth mentioning that most of the problems addressed can be solved in closed form as well. Techniques such as Linear Regression, Regularization through Ridge and Lasso Regression, and Nearest Neighbor/Kernal Regression will be addressed.\n",
    "\\\\ \\\\\n",
    "\\texttt{{\\small All the code presented here can be referenced at https://github.com/redonelima/regression}}\n",
    "\\\\ \\\\\n",
    "\\subsection{\\emph{Notation and software packages used}}\n",
    "\\centerline{\\Huge{TODO}}\n",
    "\n",
    "\\newpage\n",
    "\n",
    "\n",
    "\\section{Regression}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\subsection{\\emph{What is Regression?}}\n",
    "From a high level perspective, regression is the way to learn about a relationship between \\textbf{\\textit{features}} and \\textbf{\\textit{predictions}}. Throughout this case study approtch we will be looking at the use-case of predicting housing prices using regressive learning but the scope of regression goes well beyond this, and many of the techniques used throughout the experiments have impacts on other areas of machine learning besides regression such as bias-varience tradeoff which will be covered in depth later on. In the context of regression, the \\textbf{\\textit{features}} of a model are considered the independant variables and those\n",
    "are derived from the data. For example, for our housing predictive case, the features could be squarefeet,\n",
    "number of bathrooms, lot size, ect. These independant variables are what determine the value of our\n",
    "dependant, \\textbf{\\textit{predictive}} values, which in our example would be the value or predicted selling price of the home. It is the responsibility of the regressive model to learn the relationship between the independant and\n",
    "dependant variables.\n",
    "\n",
    "\\section{Linear Regression}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\subsection{\\emph{Simple Linear Regression}}\n",
    "The idea of simple linear regression is to fit a line to a dataset. In the simplest form, the regression line is simply a line and intercept. Even if the complexity of the model pushes into a higher order polynomial, the main problem is figuring out which line approximates the best prediction to the test data. In the examples, a dataset of house sales in Kings County Seattle and their features will be used. We start by splitting the dataset into a training set and a test set. We do this because we can fit our model to fit the data perfectly but then any variation in future predictions will be very far from the actual value and the housing price won't be well represented. Therefore we must have a portion of our data serperated from the training set to test the quality of our model. \\\\\n",
    "In a simple linear model, the obvoius equation of the model will be, \n",
    "\\begin{equation} \n",
    "f(x) = w_0 + w_1 x\n",
    "\\end{equation}\n",
    "where the equation for the true point can be represented by the equation,\n",
    "\\begin{equation}\n",
    "y_i = w_0 + w_1 x_i + \\epsilon\n",
    "\\end{equation}\n",
    "where $\\epsilon$ represents the noise that causes variation from the model. This noise will be covered more in depth in the section covering performance assessments. \\\\\n",
    "The next point to address is to figure out how to form a metric to assess the quality of the model. The first way we will address this is called the Residual Sum of Squares (RSS), which is the squared sum of the difference between the real data point and the estimated point given by the prediction model.\n",
    "\\begin{equation}\n",
    "RSS(w_0, w_1) = \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\n",
    "\\end{equation}\n",
    "\\centerline{Where $\\hat{y_i} = w_0 + w_1 x_i$ is the value predicted from the model}\n",
    "\\\\ \\\\\n",
    "In the housing example, the most obvious feature to use would be the square footage of the house, represented by the column 'sqft living' in the dataset. In the simple slope-intercept form, the $w_1$ slope term represents the change in price of the house for every unit of squarefeet. \n",
    "\n",
    "\\subsection{\\emph{Gradient Decent}}\n",
    "So the next step in finding the lowest error is minimizing the RSS. To do this we have to realize that a plot of every possible fit line will lead to a convex with the optimal point at the minimum. The gradient of a function is the derivitive vector in the increasing direction. So in order to find the minumum would be to subtract the gradient. This is known as the gradient decent algorthim. Given some function $g(w)$, when we take the derivitive with respect to $w$, then when $\\frac{dg(w)}{w}$ is negitive, it means we need to increase $w$ to reach the minimum and when the sign switches it means we need to decrease $w$.\\\\\n",
    "This algorithm, known as the gradient decent algorithm, starts at a point and takes steps in the opposite direction of the gradiant. This step size, denoted as $\\eta$ controls how far each iteration of the gradient decent steps towards the minimum. In the case where the derivitive of $g(x)$ is large, then the steps could be very large and may jump back and forth across the minimum and take a very long time to converge. On the other end, if $\\eta$ is too small, then it may take a very long time to converge. This means that step size must be choosen carefully.\\\\\n",
    "There are two popular choices in picking a step size.\n",
    "\\begin{enumerate}\n",
    "\\item Fixed step size \\\\\n",
    "$\\eta = 0.1$\n",
    "\\item Decreasing step size \\\\\n",
    "$\\eta_t = \\frac{\\alpha}{t}$ \\\\\n",
    "$\\eta_t = \\frac{\\alpha}{\\sqrt{t}}$\n",
    "\\end{enumerate}\n",
    "\n",
    "\\noindent In a closed form solution, we would know that we would reach convergence when $\\frac{dg(w)}{dw}$ is zero. Since in a real world situation, with our step size $\\eta$, it could be impossible to reach zero within a reasonable amount of time. So we have to have looser convergence criteria. Therefore, a threshold criteria $\\varepsilon$ has to be established so that when $\\frac{dg(w)}{dw} < \\varepsilon$ we can say we're close enough within convergence to say that we can consider it convered. \\\\ \\\\\n",
    "\n",
    "\\noindent This gradient decent algorithm should look like: \\\\\n",
    "\\\\\n",
    "while $\\frac{dg(w)}{dw} \\geq \\varepsilon $: \\\\\n",
    "\\indent $w^{(t+1)} = w^t - \\eta \\frac{dg(w^t)}{dw}$\n",
    "\\\\ \\\\\n",
    "The final part of the gradient decent algorithm for our linear regression model is finding $\\frac{dg(w)}{dw}$ where $g(w_i) =  (y_i - w_0 + w_1 x_i)^2$ and since the sum of the derivitive is the derivitive of the sum we can take the derivitive of a single value with respect to $w_0$ and $w_1$ to create the gradient vector.\n",
    "\\begin{equation}\n",
    "\\bigtriangledown RSS(w_0, w_1) = \\begin{vmatrix} -2\\sum_{i=1}^{N} [y_i - \\hat{y}_i] \\\\ \\\\ -2\\sum_{i=1}^{N} [y_i - \\hat{y}_i] x_i \\end{vmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "sales = graphlab.SFrame('kc_house_data.gl/')\n",
    "train_data,test_data = sales.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\\subsection{\\emph{Multiple Regression}}\n",
    "TODO: talk about Multiple Regression\n",
    "\\newpage\n",
    "\n",
    "\n",
    "\\section{Assessing Performance}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "%\\\\ \\\\\n",
    "%\\section{Regularization}\n",
    "%\\rule{\\textwidth}{2pt}\n",
    "%\\subsection{\\emph{Ridge Regression}}\n",
    "%\\subsection{\\emph{Feature Selection & Lasso Regression}}\n",
    "%\\\\ \\\\\n",
    "%\\section{Nearest Neighbor & Kernal Regression}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
