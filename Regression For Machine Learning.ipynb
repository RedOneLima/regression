{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{center}\n",
    "\\author{Kyle Hewitt}\n",
    "\\end{center}\n",
    "\\pagenumbering{roman}\n",
    "\\newpage\n",
    "\n",
    "\\section{Introduction}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\pagenumbering{arabic}\n",
    "\n",
    "Regression Machine Learning is a way to create predictive models from exsisting data to predict outcomes of new, never before seen data. This style of machine learning uses exsisting data to train and access the quality of the predicted model by splitting the exsisting data into training, validation and testing sets. As we will see when exploring farther into the different regression models, it is very easy to overfit a model to the training data so that the model looks like its perfect for the information its given but when new data is introducted, the predictions can be wildly wrong and sparatic. The idea of Regression models is to use the training data to create a model and then use the validation and testing sets to verify that these models are not overfit and how well they estimate on data outside the training set. There are different error metrics used throughout to test this idea of how well the model fits such as Residual Sum of Squares and RSME. In our experimentation we will be using the gradiant decent and coodinate decent algorithms in order to find the optimal model parameters, but it is worth mentioning that most of the problems addressed can be solved in closed form as well. Techniques such as Linear Regression, Regularization through Ridge and Lasso Regression, and Nearest Neighbor/Kernal Regression will be addressed.\n",
    "\\\\ \\\\\n",
    "\\texttt{{\\small All the code presented here can be referenced at https://github.com/redonelima/regression}}\n",
    "\\\\ \\\\\n",
    "\\subsection{\\emph{Notation and software packages used}}\n",
    "\\subsubsection{Software Packages Used}\n",
    "Throughout there will usually be two implementations of each idea, one that uses GraphLab and SFrame and another that will be implemented myself. What to expect from this is to see the behavior with these exsisting tools and then xperimentation in self implementation. Other tools can be used such as sypi and pandas, but this course uses GraphLab because it is good for large datasets because it doesn't require the entire data set to be in memory, instead it only pulls the data into memory as it is needed. This causes the import of GraphLab and the import of the Frame to take a while to load, however, once this is completed operations run faster and large-scale data sets can be used. In the example of the kc\\_house\\_data.gl set has 17,258 house entries, each with 21 features. \\\\ \\\\\n",
    "The Numpy library is used for complex matrix calculations. Numpy is a python library which is written in fortran for extremely efficent matrix calculations. These calculations could be done manually in python, but given large data sets, this would be very slow and inefficent.\n",
    "The MathPlotLib python library is used to visulize data throughout. Graphlab also has functions like .show() to visualize data. \\\\ \\\\\n",
    "The following import will be used throughout most of the code examples thoughout:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1515530009.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to khewitt08@live.com and will expire on December 28, 2018.\n"
     ]
    }
   ],
   "source": [
    "import graphlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "graphlab.canvas.set_target('ipynb', port=None)\n",
    "sales = graphlab.SFrame('kc_house_data.gl/')\n",
    "train_data,test_data = sales.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "\n",
    "\\section{Regression}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\subsection{\\emph{What is Regression?}}\n",
    "From a high level perspective, regression is the way to learn about a relationship between \\textbf{\\textit{features}} and \\textbf{\\textit{predictions}}. Throughout this case study approtch we will be looking at the use-case of predicting housing prices using regressive learning but the scope of regression goes well beyond this, and many of the techniques used throughout the experiments have impacts on other areas of machine learning besides regression such as bias-varience tradeoff which will be covered in depth later on. In the context of regression, the \\textbf{\\textit{features}} of a model are considered the independant variables and those\n",
    "are derived from the data. For example, for our housing predictive case, the features could be squarefeet,\n",
    "number of bathrooms, lot size, ect. These independant variables are what determine the value of our\n",
    "dependant, \\textbf{\\textit{predictive}} values, which in our example would be the value or predicted selling price of the home. It is the responsibility of the regressive model to learn the relationship between the independant and\n",
    "dependant variables.\n",
    "\n",
    "\\subsection{Credits and material citation}\n",
    "All the material covered comes from the University of Washington's machine learning specialization on Coursera. Original material can be found on the  same github resource page as the source code. The videos are only avalible through Coursea. All of the coding examples throughout the book come from assignments from this course. Some base code was provided via ipython notebooks. All implimentations within the base code are original work.\\\\ \\\\\n",
    "\\texttt{{\\small All references can be found at https://github.com/RedOneLima/regression/tree/master/references}}\n",
    "\n",
    "\\newpage\n",
    "\n",
    "\\section{Linear Regression}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\subsection{\\emph{Simple Linear Regression}}\n",
    "The idea of simple linear regression is to fit a line to a dataset. In the simplest form, the regression line is simply a line and intercept. Even if the complexity of the model pushes into a higher order polynomial, the main problem is figuring out which line approximates the best prediction to the test data. In the examples, a dataset of house sales in Kings County Seattle and their features will be used. We start by splitting the dataset into a training set and a test set. We do this because we can fit our model to fit the data perfectly but then any variation in future predictions will be very far from the actual value and the housing price won't be well represented. Therefore we must have a portion of our data serperated from the training set to test the quality of our model. \\\\\n",
    "In a simple linear model, the obvoius equation of the model will be, \n",
    "\\begin{equation} \n",
    "f(x) = w_0 + w_1 x\n",
    "\\end{equation}\n",
    "where the equation for the true point can be represented by the equation,\n",
    "\\begin{equation}\n",
    "y_i = w_0 + w_1 x_i + \\epsilon\n",
    "\\end{equation}\n",
    "where $\\epsilon$ represents the noise that causes variation from the model. This noise will be covered more in depth in the section covering performance assessments. \\\\\n",
    "The next point to address is to figure out how to form a metric to assess the quality of the model. The first way we will address this is called the Residual Sum of Squares (RSS), which is the squared sum of the difference between the real data point and the estimated point given by the prediction model.\n",
    "\\begin{equation}\n",
    "RSS(w_0, w_1) = \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\n",
    "\\end{equation}\n",
    "\\centerline{Where $\\hat{y_i} = w_0 + w_1 x_i$ is the value predicted from the model}\n",
    "\\\\ \\\\\n",
    "In the housing example, the most obvious feature to use would be the square footage of the house, represented by the column 'sqft living' in the dataset. In the simple slope-intercept form, the $w_1$ slope term represents the change in price of the house for every unit of squarefeet. \n",
    "\n",
    "\n",
    "\\subsection{\\emph{Gradient Descent}}\n",
    "So the next step in finding the lowest error is minimizing the RSS. To do this we have to realize that a plot of every possible fit line will lead to a convex with the optimal point at the minimum. The gradient of a function is the derivitive vector in the increasing direction. So in order to find the minumum would be to subtract the gradient. This is known as the gradient decent algorthim. Given some function $g(w)$, when we take the derivitive with respect to $w$, then when $\\frac{dg(w)}{w}$ is negitive, it means we need to increase $w$ to reach the minimum and when the sign switches it means we need to decrease $w$.\\\\\n",
    "This algorithm, known as the gradient decent algorithm, starts at a point and takes steps in the opposite direction of the gradiant. This step size, denoted as $\\eta$ controls how far each iteration of the gradient decent steps towards the minimum. In the case where the derivitive of $g(x)$ is large, then the steps could be very large and may jump back and forth across the minimum and take a very long time to converge. On the other end, if $\\eta$ is too small, then it may take a very long time to converge. This means that step size must be choosen carefully.\\\\\n",
    "There are two popular choices in picking a step size.\n",
    "\\begin{center}\n",
    "\\begin{enumerate}\n",
    "\\item Fixed step size \\\\\n",
    "$\\eta = 0.1$\n",
    "\\item Decreasing step size \\\\\n",
    "$\\eta_t = \\frac{\\alpha}{t}$ \\\\\n",
    "$\\eta_t = \\frac{\\alpha}{\\sqrt{t}}$\n",
    "\\end{enumerate}\n",
    "\\end{center}\n",
    "\n",
    "\\noindent In a closed form solution, we would know that we would reach convergence when $\\frac{dg(w)}{dw}$ is zero. Since in a real world situation, with our step size $\\eta$, it could be impossible to reach zero within a reasonable amount of time. So we have to have looser convergence criteria. Therefore, a threshold criteria $\\varepsilon$ has to be established so that when $\\frac{dg(w)}{dw} < \\varepsilon$ we can say we're close enough within convergence to say that we can consider it convered. \\\\ \\\\\n",
    "\n",
    "\\noindent This gradient decent algorithm should look like: \\\\\n",
    "\\\\\n",
    "\\centerline{while $\\frac{dg(w)}{dw} \\geq \\varepsilon $:} \\\\\n",
    "\\centerline{\\indent $w^{(t+1)} = w^t - \\eta \\frac{dg(w^t)}{dw}$}\n",
    "\\\\ \\\\\n",
    "The final part of the gradient decent algorithm for our linear regression model is finding $\\frac{dg(w)}{dw}$ where $g(w_i) =  (y_i - w_0 + w_1 x_i)^2$ and since the sum of the derivitive is the derivitive of the sum we can take the derivitive of a single value with respect to $w_0$ and $w_1$ to create the gradient vector.\n",
    "\\begin{equation}\n",
    "\\bigtriangledown RSS(w_0, w_1) = \\begin{vmatrix} -2\\sum_{i=1}^{N} [y_i - \\hat{y}_i] \\\\ \\\\ -2\\sum_{i=1}^{N} [y_i - \\hat{y}_i] x_i \\end{vmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "This ends up being a vector because even in our linear equation we have two variables. This will be a very important concept when we look at multiple regression in the next section where we start adding other features like number of bathrooms or bedrooms ontop of square footage. Given this vector, we can used a closed form solution to solve for each of the unknowns in closed form:\n",
    "\\begin{equation}\n",
    "\\hat{w}_0 = \\frac{\\sum y_i}{N} - \\hat{w}_1 \\frac{\\sum x_i}{N}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\hat{w}_1 = \\frac{\\sum y_i x_i - \\frac{\\sum y_i x_i}{N}}{\\sum x_i ^2- \\frac{\\sum x_i \\sum x_i}{N}}\n",
    "\\end{equation}\n",
    "\n",
    "Now that we have these closed form solutions for these two variables. There's a lot of similarities between these two equations so we can pull out a few main calculations that we need, namely:\n",
    "\\\\ \\\\\n",
    "\\indent \\indent \\indent \\indent \\indent \\indent \\indent\n",
    "\\indent 1. $\\sum y_i$\n",
    "\\indent 2. $\\sum x_i$\n",
    "\\indent 3. $\\sum y_i x_i$\n",
    "\\indent 4. $\\sum x_i^2$ \n",
    "\\\\ \\\\\n",
    "Using these, we can create a function for calculating simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_linear_regression(input_feature, output):\n",
    "    # compute the sum of input_feature and output\n",
    "    feature_sum = input_feature.sum()\n",
    "    output_sum = output.sum()\n",
    "    num_inputs = output.size()\n",
    "    \n",
    "    # compute the product of the output and the input_feature and its sum\n",
    "    product_sum = (input_feature *output).sum()\n",
    "    \n",
    "    # compute the squared value of the input_feature and its sum\n",
    "    squared_sum = (input_feature ** 2).sum()\n",
    "    # use the formula for the slope\n",
    "    numerator = product_sum - ((float(1)/num_inputs) * (feature_sum*output_sum))\n",
    "    denominator = squared_sum - ((float(1)/num_inputs) * (feature_sum*feature_sum))\n",
    "    slope = numerator/denominator\n",
    "    # use the formula for the intercept\n",
    "    intercept = output.mean() - slope * input_feature.mean()\n",
    "    return (intercept, slope)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we can test this function by giving it input that we know the outcome. \\\\\n",
    "We are going to make a feature and then put the output exactly on a line. This results in both our slope and interscept being 1. It will also be used to test our model since all the points will fall on a line it means out RSS will be exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 1.0\n",
      "Slope: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_feature = graphlab.SArray(range(5))\n",
    "test_output = graphlab.SArray(1 + 1*test_feature)\n",
    "(test_intercept, test_slope) =  simple_linear_regression(\n",
    "    test_feature, test_output)\n",
    "print \"Intercept: \" + str(test_intercept)\n",
    "print \"Slope: \" + str(test_slope)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As expected, we see that both slope and intercept of our simple test case are 1. \\\\ Next, its time to use our simple linear regression model on some actual data. As mentioned earlier our feature for the focus of this model is going to be the square footage of the house and our predictive $\\hat{y}$ is going to be the sales price of the house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -47116.0765749\n",
      "Slope: 281.958838568\n"
     ]
    }
   ],
   "source": [
    "sqft_intercept, sqft_slope = simple_linear_regression(\n",
    "    train_data['sqft_living'], train_data['price'])\n",
    "\n",
    "print \"Intercept: \" + str(sqft_intercept)\n",
    "print \"Slope: \" + str(sqft_slope)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the above code we can see that we have an intercept approximately $-47,116$ and a slope of approximately $282$. \\\\\n",
    "What this means for us is that a house with no square feet (an empty lot) would actually cost the seller $\\$ 47,000$ for someone to take. Clearly, this intercept has very little significance for our purposes.\n",
    "\\\\\n",
    "However, the slope value of 281.96 is significant and it means that for every square foot extra a house has, the sales price of the house increases by about $\\$ 282$. Clearly this is not the most accurate model, since there are many other factors that can play into the value of a home, but that's the reason we're calling this a 'simple' model. \n",
    "\\\\ \\\\\n",
    "Now that we have a model, we can start making predictions.\n",
    "\\subsection{Code Example}\n",
    "\\rule{\\textwidth}{2pt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_regression_predictions(input_feature, intercept, slope):\n",
    "    # calculate the predicted values:\n",
    "    return  slope*input_feature + intercept"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have our model and our prediction function we can now use it to predict new houses based on the square footage feature. Lets look at a house that has a size of 2650 sqft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated price for a house with 2650 squarefeet is $700074.85\n"
     ]
    }
   ],
   "source": [
    "my_house_sqft = 2650\n",
    "estimated_price = get_regression_predictions(\n",
    "    my_house_sqft, sqft_intercept, sqft_slope)\n",
    "print \"The estimated price for a house with %d squarefeet is $%.2f\" % (\n",
    "    my_house_sqft, estimated_price)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we can see, using our predictive model a house with 2650 sqft is estimated to be sold for \\$ 700,000. \\\\ \\\\\n",
    "Lets take a look at the quality of our model. We will calculate our RSS, which we already know should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_residual_sum_of_squares(input_feature, output, intercept, slope):\n",
    "    # First get the predictions\n",
    "    predicted_price = get_regression_predictions(input_feature, intercept,slope)\n",
    "    # then compute the residuals (since we are squaring it doesn't matter which order you subtract)\n",
    "    residual = output - (slope*input_feature+intercept)\n",
    "    # square the residuals and add them up\n",
    "    RSS = residual * residual\n",
    "    return(RSS.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print get_residual_sum_of_squares(\n",
    "    test_feature, test_output, test_intercept, test_slope) \n",
    "# should be 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RSS of predicting Prices based on Square Feet is : 1.20191835632e+15\n"
     ]
    }
   ],
   "source": [
    "rss_prices_on_sqft = get_residual_sum_of_squares(\n",
    "    train_data['sqft_living'], train_data['price'], \n",
    "    sqft_intercept, sqft_slope)\n",
    "print 'The RSS of predicting Prices based on Square Feet is : ' + str(\n",
    "    rss_prices_on_sqft)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can use this to go the other way as well. Given a price, we can tell how big of a house that a buyer can afford."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_regression_predictions(output, intercept, slope):\n",
    "    return  (output-intercept)/slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated squarefeet for a house worth $800000.00 is 3004\n"
     ]
    }
   ],
   "source": [
    "my_house_price = 800000\n",
    "estimated_squarefeet = inverse_regression_predictions(\n",
    "    my_house_price, sqft_intercept, sqft_slope)\n",
    "print \"The estimated squarefeet for a house worth $%.2f is %d\" % (\n",
    "    my_house_price, estimated_squarefeet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So we can use this model to say that a buyer with $\\$ 800,000$ could afford a house up to $3,004 sqft$. \n",
    "\\\\ \\\\\n",
    "Now that we have a simple linear model on our housing data, we can change up the features and see how it fits a model when looking at bedrooms and see how that compares to the prediction of square footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 109473.180469\n",
      "Slope: 127588.952175\n"
     ]
    }
   ],
   "source": [
    "bedrooms_intercept, bedrooms_slope = simple_linear_regression(\n",
    "    train_data['bedrooms'], train_data['price'])\n",
    "\n",
    "print \"Intercept: \" + str(bedrooms_intercept)\n",
    "print \"Slope: \" + str(bedrooms_slope)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So based on our bedroom model, we can see that we get an increase in housing price of \\$ 127,589 per bedroom. So now that we have our two training models, its time to see how well these predict our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RSS of predicting Prices based on Bedrooms is : 4.93364582868e+14\n"
     ]
    }
   ],
   "source": [
    "# Compute RSS when using bedrooms on TEST data:\n",
    "rss_prices_on_bedrooms_test = get_residual_sum_of_squares(\n",
    "    test_data['bedrooms'], test_data['price'],\n",
    "    bedrooms_intercept, bedrooms_slope)\n",
    "print 'The RSS of predicting Prices based on Bedrooms is : ' + str(\n",
    "    rss_prices_on_bedrooms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RSS of predicting Prices based on Square Feet is : 2.75402936247e+14\n"
     ]
    }
   ],
   "source": [
    "# Compute RSS when using squarfeet on TEST data:\n",
    "rss_prices_on_sqft_test = get_residual_sum_of_squares(\n",
    "    test_data['sqft_living'], test_data['price'], \n",
    "    sqft_intercept, sqft_slope)\n",
    "print 'The RSS of predicting Prices based on Square Feet is : ' + str(\n",
    "    rss_prices_on_sqft_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have our RSS of our squarefoot model and our bedroom model, we can comare our two RSS and see that since the squarefootage model has a lower RSS on the testing data it does a better job of a predictive model. This is because we trained our model on the training data, so all of the houses in the testing set are all houses that the model has never seen. Since the RSS of squarefeet is lower, it means that the houses in the testing with the squarefeet feature set were all closer to the prediction model than those of the bedroom feature set.\n",
    "\\\\ \\\\\n",
    "It is important at this point to address the fact that this model is assuming an asyemetric error, meaning that we're assuming the same consiquence for under estimating than over estimating. With housing, this may not be the case. A house whos value is over estimated may not get many offers and therefore take longer to sell. A house that is underestimated may listed too low and the buyer ends up losing money. The consequences of each of these errors are actually asymmetric, but this adds an entirely new level of complexity so we will be treating errors symmetricly. \n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\section{Multiple Regression}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "Anyone who has ever had to deal with valueing a home knows there's a lot more to it than just the square footage or the number of bedrooms by themselves. In this section we will be looking at multiple regression, meaning that we're going to be building a regression model based off of multiple features and not just off a single one. We will also be looking at what is known as polynomial regression which will allow us to fit our model to a higher order polynomial, giving our model much more flexibility. \\\\\n",
    "\\\\\n",
    "There are not always exponential relationships between features and their trends. An example of a exponential polynomial equation could be $y_i = w_0 + w_2 t_i + w_3 t_i^2 + ... + w_n t_i^n$ where as a more complex model that has a linearly upward trend could look like $y_i = w_0 + w_1 t_i + w_2 \\sin{( \\frac{2 \\pi t_i}{12} - \\phi )} + \\epsilon _i$ \\\\\n",
    "This second equation represents seasonality in a linearly upward trend, which is a good representitive model of anything that has variying trends throughout the year but has an overall upward trend. Examples of this are camping equipemnt sales that have higher sales thoughout the summer during the camping season. Clearly this will give an overall better prediction than a linear model, but this is much harder to model. This is where the power of regressive learning and gradient decent start to really show their power.\n",
    "\\subsection{\\emph{The Polynomial Regression Model}}\n",
    "We are going to look at this in general form, also known as the generic basis expansion. Each $j^{th}$ feature will have a bias, as seen with the example of the $\\sin$ function above, which we will refer to as $h_j (x_i)$ which is the $j^{th}$ feature of data input i refered to as $x_i$.\n",
    "\\\\\n",
    "So the generic basis expansion for each $x_i$ data input:\n",
    "\\begin{equation}\n",
    "\\sum_{j=0}^{D} w_j h_j (x_i) + \\epsilon_i\n",
    "\\end{equation}\n",
    "\\centerline{Where D is the number of features for each data point}\n",
    "This means that instead of being in a 2 dementional space like we were when we were looking at a single feature, we will be looking at a D+1 dimentional space when looking at multiple features. So for a model that accounts for 2 features, we'd be looking at a hyperplane in a 3D space instead of a line in a 2D space. This is also known as a D-dimensional curve. \\\\ \\\\\n",
    "So how do we look at this regression model in this new D-dimentional space? We have to start looking at the data points in matrix notation since we're dealing with an $n \\times m$ matrix of observations which we will refer to as $\\textbf{H}$ where $n$ is the number of features $D$ and $m$ is the number of observations $i$. \\\\ $\\textbf{H}$ is the multiplied by the $D \\times 1$ feature vector $\\vec{w}$. Finally we add in our error vector $\\vec{\\epsilon}$. Finally this will give us our final vector $\\vec{y}$.\n",
    "\\begin{equation}\n",
    "\\vec{y} = \\textbf{H} \\vec{w} + \\vec{\\epsilon}\n",
    "\\end{equation}\n",
    "We will start by looking at an instance of $D$ observations, each with a single feature, like in our linear regression model. because of the rules of vector multiplcation, we will take the transpose of this observation/feature vector, denoted $\\vec{h}$. This is then multiplied by the coefficent vector $\\vec{w}$ and added to our single $\\epsilon$ value. \n",
    "\\begin{equation}\n",
    "y_i = \\vec{h}^T(x_i) \\vec{w} + \\epsilon\n",
    "\\end{equation}\n",
    "So our vectors for a single iteration would look like:\n",
    "\\begin{equation}\n",
    "y_i = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "w_0 & w_1 & w_2 & \\ldots & w_D\n",
    "\\end{array}\n",
    "\\right] + \\left[\n",
    "\\begin{array}{c}\n",
    "h_0(x_i) \\\\\n",
    "h_1(x_i) \\\\\n",
    "h_2(x_i) \\\\\n",
    "\\vdots \\\\\n",
    "h_D(x_i)\n",
    "\\end{array}\n",
    "\\right]\n",
    "+ [\\epsilon]\n",
    "\\end{equation}\n",
    "\n",
    "So lets look at this as a matrix of $N$ observations:\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_N\n",
    "\\end{array}\n",
    "\\right] = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "h_0(x_1) & h_1(x_1) & \\ldots & h_D(x_1) \\\\\n",
    "h_0(x_2) & h_1(x_2) & \\ldots & h_D(x_2) \\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots   \\\\\n",
    "h_0(x_N) & h_1(x_N) & \\ldots & h_D(x_N)\n",
    "\\end{array}\n",
    "\\right]  \\left[\n",
    "\\begin{array}{c}\n",
    "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D\n",
    "\\end{array}\n",
    "\\right] + \\left[\n",
    "\\begin{array}{c}\n",
    "\\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_N\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\subsection{\\emph{Computing the RSS}}\n",
    "Now that we have our multiple feature regression model, we need to figure out how we're going to assess the cost of a given model in order to find the one that best fits our predictive model. Just like we did in our simple linear model, we will use RSS as a metric for this cost. Just like with our model we will write the RSS in terms of a single component and then scale it up. Since we know that the RSS is the difference between the actual value and the predicted value, our first part of the RSS equation will remain the same. What will change is the definition of our $\\hat{y}$ value. We will write this in terms of the model itself this time instead of in terms of $\\hat{y}$ because it will be easier to scale it up into matrix form. \n",
    "\\begin{equation}\n",
    "RSS(\\vec{w}) = \\sum_{i=1}^{N} (y_i - \\vec{h}^T(x_i) \\vec{w})^2\n",
    "\\end{equation}\n",
    "Since we know that $\\vec{h}(x_i)$ is a single component of $\\textbf{H}$, then the matrix form of $\\vec{h}(x_i) \\vec{w}$ would be $\\textbf{H}\\vec{w}$ and to square each component in the resulting vector we would multiply the resulting vecor by its transpose. Therefore our matrix notation of the RSS would be:\n",
    "\\begin{equation}\n",
    "RSS(\\vec{w}) = \\sum_{i=1}^{N} (y - \\textbf{H}\\vec{w})^T (y - \\textbf{H}\\vec{w})\n",
    "\\end{equation}\n",
    "\n",
    "\\subsection{\\emph{Gradient of the RSS}}\n",
    "Now that we have our model for our RSS, we need to find the gradient in order to find our lowest cost model. The gradient is much more useful in this polynomial regression model than it did in the linear model because now we have more features. To find our gradient we will take the same steps as we did in earlier derivations and solve for our gradient in 1D and scale it up. To find the dirivitive of our 1D case we must note that both $y$, $h$ and $w$ are scalers in our 1D case.\n",
    "\\begin{eqnarray}\n",
    "\\frac{d}{dw} (y-hw)(y-hw) & = &\\frac{d}{dw} (y-hw)^2 \\\\\n",
    "                          & = &2(y-hw)^1 (-h) \\nonumber \\\\\n",
    "                          & = &-2h(y-hw) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\\centerline{So its not hard to see that when its scaled up our gradient is:}\n",
    "\\begin{eqnarray}\n",
    "\\bigtriangledown RSS(\\vec{w}) &= &\\bigtriangledown [(\\vec{y} - \\textbf{H}\\vec{w})^T (\\vec{y} - \\textbf{H}\\vec{w}) \\\\\n",
    "                              &= &-2\\textbf{H}^T(\\vec{y}-\\textbf{H}\\vec{w}) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\subsubsection{Closed Form Solution}\n",
    "Just like in our linear model, we can solve for our polynomial model in the same closed form way. Now that we have our gradient of our RSS, we can set it equal to zero and solve for $\\hat{w}$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "-2\\textbf{H}^T(\\vec{y}-\\textbf{H}\\vec{\\hat{w}}) &= &0 \\\\\n",
    "-\\not{2}\\textbf{H}^T\\vec{y} + \\not{2}\\textbf{H}^T \\textbf{H}\\vec{w}) &= &0 \\nonumber \\\\\n",
    "\\textbf{H}^T \\textbf{H}\\vec{\\hat{w}} &  = &\\textbf{H}^T\\vec{y} \\nonumber \\\\\n",
    "(\\textbf{H}^T\\textbf{H})^{-1} \\textbf{H}^T \\textbf{H}\\vec{\\hat{w}} & = &(\\textbf{H}^T\\textbf{H})^{-1} \\textbf{H}^T\\vec{y} \\nonumber \\\\\n",
    "\\textbf{I}\\vec{\\hat{w}} &= &(\\textbf{H}^T\\textbf{H})^{-1} \\textbf{H}^T\\vec{y} \\nonumber \\\\\n",
    "\\vec{\\hat{w}} &= &(\\textbf{H}^T\\textbf{H})^{-1} \\textbf{H}^T\\vec{y} \\nonumber \n",
    "\\end{eqnarray}\n",
    "\n",
    "This solved out very smoothly but there's a few problems when approching this in a closed form way like we did with the linear model. Both of the main issues have to do with the inverse. First, $(\\textbf{H}^T\\textbf{H})^{-1}$ is only invertable if, in most cases, the number of linearly independant observations N \\textgreater D. This limits the complexity of the models we can make. This issue is addressed later with regularization. The second issue is the complexity of doing the inverse. The complexity of inverting a $D \\times D$ matrix is $\\Omega(D^3)$ which can get very compulationaly expensive with large data sets. Therefore using this method is only an option on reasonably small sets of data. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\subsubsection{Gradient Descent}\n",
    "The gradient descent for polynomial regression isn't much different from linear regression, just accounting for more features. The first difference is the convergence criteria. Now that we have many other features accounted for in the gradient, now we must find the minimum of each of those features. The value cost of each feature is in the RSS vector. Therefore the convergence of the function would be when the magnitude of the gradient of the RSS is zero, or within the threshold $\\varepsilon$ in our real world case. More formally, $\\|\\bigtriangledown RSS(\\vec{w}^{(t)})\\|  \\textgreater  \\varepsilon$\n",
    "\\\\\n",
    "Then for each of the features, refered to as partials here, the sum of the dirivitive of the partials are multiplied by the step size $\\eta$ for each of the iterations $t$. \n",
    "\\\\ \\\\\n",
    "while $\\| \\bigtriangledown RSS(\\vec{w}^t) \\| \\textgreater \\varepsilon:$ \\\\\n",
    "\\indent for j in range(0, D): \\\\\n",
    "\\indent \\indent $partial_j = -2\\sum_{i=1}^{N} h_j(\\vec{x}_i)(y_i -                     \\hat{y}_i(\\vec{w}^{(t)}))$ \\\\\n",
    "\\indent \\indent$\\vec{w}_j^{(t+1)} = \\vec{w}_j^{t} - \\eta (partial_j)$ \\\\\n",
    "t++\n",
    "\\subsection{Code Example}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "We will start out with using the GraphLab linear\\_regression model again. This time however we will be passing it a list of features which will cause it to run polynomial regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Linear regression:</pre>"
      ],
      "text/plain": [
       "Linear regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 17384</pre>"
      ],
      "text/plain": [
       "Number of examples          : 17384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of features          : 3</pre>"
      ],
      "text/plain": [
       "Number of features          : 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 3</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 4</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 1.049275     | 4146407.600631     | 258679.804477 |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 1.049275     | 4146407.600631     | 258679.804477 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_features = ['sqft_living', 'bedrooms', 'bathrooms']\n",
    "example_model = graphlab.linear_regression.create(\n",
    "    train_data, target = 'price', features = example_features, \n",
    "                                                  validation_set = None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We now have a trained model. From here we can look at the coefficents of the model and make predictions based off the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+---------------+\n",
      "|     name    | index |     value      |     stderr    |\n",
      "+-------------+-------+----------------+---------------+\n",
      "| (intercept) |  None | 87910.0724924  |  7873.3381434 |\n",
      "| sqft_living |  None | 315.403440552  | 3.45570032585 |\n",
      "|   bedrooms  |  None | -65080.2155528 | 2717.45685442 |\n",
      "|  bathrooms  |  None | 6944.02019265  | 3923.11493144 |\n",
      "+-------------+-------+----------------+---------------+\n",
      "[4 rows x 4 columns]\n",
      "\n",
      "271789.505878\n"
     ]
    }
   ],
   "source": [
    "example_weight_summary = example_model.get(\"coefficients\")\n",
    "print example_weight_summary\n",
    "example_predictions = example_model.predict(train_data)\n",
    "print example_predictions[0] # should be 271789.505878"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have a trained model we can compute the cost metric using RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_residual_sum_of_squares(model, data, outcome):\n",
    "    # First get the predictions\n",
    "    predictions = model.predict(data)\n",
    "    # Then compute the residuals/errors\n",
    "    errors = outcome - predictions\n",
    "    # Then square and add them up\n",
    "    RSS = (errors ** 2).sum()\n",
    "    return(RSS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7376153833e+14\n"
     ]
    }
   ],
   "source": [
    "rss_example_train = get_residual_sum_of_squares(example_model, test_data, test_data['price'])\n",
    "print rss_example_train # should be 2.7376153833e+14"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we've varified that our model is trained, we can add a few more featured based off of the features we already have. The reason for this is that features are somtimes codependant and more accurate models can be created by a combonation of other features. For this experiment we will be  the square of the bedroom count, the product of bedrooms and bathrooms, the log of the sqare feet, and the sum of the properties latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "train_data['bedrooms_squared'] = train_data['bedrooms'].apply(lambda x: x**2)\n",
    "train_data['bed_bath_rooms'] = train_data['bedrooms'] * train_data['bathrooms']\n",
    "train_data['log_sqft_living'] = train_data['sqft_living'].apply(lambda x: log(x))\n",
    "train_data['lat_plus_long'] = train_data['lat'] + train_data['long']\n",
    "\n",
    "test_data['bedrooms_squared'] = test_data['bedrooms'].apply(lambda x: x**2)\n",
    "test_data['bed_bath_rooms'] = test_data['bedrooms'] * test_data['bathrooms']\n",
    "test_data['log_sqft_living'] = test_data['sqft_living'].apply(lambda x: log(x))\n",
    "test_data['lat_plus_long'] = test_data['lat'] + test_data['long']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The purpose of squaring the bedrooms is to seperate data more between few bedrooms and many bedrooms. The product of the bedrooms and bathrooms is known as an \\emph{interaction} feature. It is large only when both bedrooms and bathroom counts are large. Taking the log of the square feet will spread out small values and bring larger values closer together. The final feature of the sum of the latitude and longitude is only to show that using poor feature selection leads to poor predictive models. \n",
    "\\\\\n",
    "For this experiment, we will make three seperate models. The first will have bedrooms, bathrooms, square feet, latitude, and laditude. All of these features exist in the orginal dataset.\\\\\n",
    "The next model will include all of the features of model 1 plus the remainder of the new features we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1_features = ['sqft_living', 'bedrooms', 'bathrooms', 'lat', 'long']\n",
    "model_2_features = model_1_features + ['bed_bath_rooms']\n",
    "model_3_features = model_2_features + ['bedrooms_squared', 'log_sqft_living', 'lat_plus_long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Linear regression:</pre>"
      ],
      "text/plain": [
       "Linear regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 17384</pre>"
      ],
      "text/plain": [
       "Number of examples          : 17384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of features          : 5</pre>"
      ],
      "text/plain": [
       "Number of features          : 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 5</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 6</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 0.100621     | 4074878.213096     | 236378.596455 |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 0.100621     | 4074878.213096     | 236378.596455 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Linear regression:</pre>"
      ],
      "text/plain": [
       "Linear regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 17384</pre>"
      ],
      "text/plain": [
       "Number of examples          : 17384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of features          : 6</pre>"
      ],
      "text/plain": [
       "Number of features          : 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 6</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 7</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 0.128063     | 4014170.932927     | 235190.935428 |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 0.128063     | 4014170.932927     | 235190.935428 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Linear regression:</pre>"
      ],
      "text/plain": [
       "Linear regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 17384</pre>"
      ],
      "text/plain": [
       "Number of examples          : 17384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of features          : 9</pre>"
      ],
      "text/plain": [
       "Number of features          : 9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 9</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 10</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 0.091382     | 3193229.177894     | 228200.043155 |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 0.091382     | 3193229.177894     | 228200.043155 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_1 = graphlab.linear_regression.create(train_data, target = 'price', \n",
    "                                            features = model_1_features, validation_set = None)\n",
    "model_2 = graphlab.linear_regression.create(train_data, target = 'price', \n",
    "                                            features = model_2_features, validation_set = None)\n",
    "model_3 = graphlab.linear_regression.create(train_data, target = 'price', \n",
    "                                            features = model_3_features, validation_set = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+---------------+\n",
      "|     name    | index |     value      |     stderr    |\n",
      "+-------------+-------+----------------+---------------+\n",
      "| (intercept) |  None | -56140675.7444 | 1649985.42028 |\n",
      "| sqft_living |  None | 310.263325778  | 3.18882960408 |\n",
      "|   bedrooms  |  None | -59577.1160682 | 2487.27977322 |\n",
      "|  bathrooms  |  None | 13811.8405418  | 3593.54213297 |\n",
      "|     lat     |  None | 629865.789485  | 13120.7100323 |\n",
      "|     long    |  None | -214790.285186 | 13284.2851607 |\n",
      "+-------------+-------+----------------+---------------+\n",
      "[6 rows x 4 columns]\n",
      "\n",
      "+----------------+-------+----------------+---------------+\n",
      "|      name      | index |     value      |     stderr    |\n",
      "+----------------+-------+----------------+---------------+\n",
      "|  (intercept)   |  None | -54410676.1152 | 1650405.16541 |\n",
      "|  sqft_living   |  None | 304.449298057  | 3.20217535637 |\n",
      "|    bedrooms    |  None | -116366.043231 | 4805.54966546 |\n",
      "|   bathrooms    |  None | -77972.3305135 | 7565.05991091 |\n",
      "|      lat       |  None | 625433.834953  | 13058.3530972 |\n",
      "|      long      |  None | -203958.60296  | 13268.1283711 |\n",
      "| bed_bath_rooms |  None | 26961.6249092  | 1956.36561555 |\n",
      "+----------------+-------+----------------+---------------+\n",
      "[7 rows x 4 columns]\n",
      "\n",
      "+------------------+-------+----------------+---------------+\n",
      "|       name       | index |     value      |     stderr    |\n",
      "+------------------+-------+----------------+---------------+\n",
      "|   (intercept)    |  None | -52974974.0602 |  1615194.9439 |\n",
      "|   sqft_living    |  None | 529.196420564  | 7.69913498511 |\n",
      "|     bedrooms     |  None | 28948.5277313  | 9395.72889106 |\n",
      "|    bathrooms     |  None |  65661.207231  | 10795.3380703 |\n",
      "|       lat        |  None | 704762.148408  | 1292011141.66 |\n",
      "|       long       |  None | -137780.01994  | 1292011141.57 |\n",
      "|  bed_bath_rooms  |  None | -8478.36410518 | 2858.95391257 |\n",
      "| bedrooms_squared |  None | -6072.38466067 | 1494.97042777 |\n",
      "| log_sqft_living  |  None | -563467.784269 | 17567.8230814 |\n",
      "|  lat_plus_long   |  None | -83217.1979248 | 1292011141.58 |\n",
      "+------------------+-------+----------------+---------------+\n",
      "[10 rows x 4 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine/extract each model's coefficients:\n",
    "print model_1.get(\"coefficients\")\n",
    "print model_2.get(\"coefficients\")\n",
    "print model_3.get(\"coefficients\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have the coefficents of our three models, we can compare what impacts the inclusion of new features has. We see a change in sign from the first model to the second, which tells us that when we add the interaction feature to the set, the bedroom coefficent goes the opposite direction. \\\\ \\\\\n",
    "Now that we have our two models, we can compute the RSS to find how well our models represent the real. We will first compute the RSS on our training data, which we can expect to be very similar, and the more complex model will have the lower training error, intuitively. Then we will run the RSS on the test set and see which model shows the best on data that it was not trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.71328233544e+14\n",
      "9.61592067856e+14\n",
      "9.05276314555e+14\n"
     ]
    }
   ],
   "source": [
    "# Compute the RSS on TRAINING data for each of the three models and record the values:\n",
    "RSS_1 = get_residual_sum_of_squares(model_1, train_data, train_data['price'])\n",
    "RSS_2 = get_residual_sum_of_squares(model_2, train_data, train_data['price'])\n",
    "RSS_3 = get_residual_sum_of_squares(model_3, train_data, train_data['price'])\n",
    "\n",
    "print RSS_1\n",
    "print RSS_2\n",
    "print RSS_3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Like expected, our RSS was lowest on the third model because it had the most features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.26568089093e+14\n",
      "2.24368799994e+14\n",
      "2.51829318952e+14\n"
     ]
    }
   ],
   "source": [
    "# Compute the RSS on TESTING data for each of the three models and record the values:\n",
    "RSS_1 = get_residual_sum_of_squares(model_1, test_data, test_data['price'])\n",
    "RSS_2 = get_residual_sum_of_squares(model_2, test_data, test_data['price'])\n",
    "RSS_3 = get_residual_sum_of_squares(model_3, test_data, test_data['price'])\n",
    "\n",
    "print RSS_1\n",
    "print RSS_2\n",
    "print RSS_3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we can see that the second model actually represented the best model with the lowest RSS on the test set. When we gave it more meaningful features, the predictive model improved and the test data fell closest to the predicted model. However, in the third model we added three more, less meaningful features (including one meaningless feature) and we see that it actually has the worst performing model. Feature selection is clearly a very important aspect of regression learning and will be covered in depth in a later section in regularization. \n",
    "\\\\ \\\\\n",
    "Now that we see how this polynomal regression behaves in GraphLab, lets implement some polynomial regression ourselves. Since we will be using numpy for our matrix calculations, we need to have a function that converts an SFrame into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted value 1181.0\n",
      "1181.0\n",
      "2571.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_numpy_data(data_sframe, features, output):\n",
    "    data_sframe['constant'] = 1 # this is how you add a constant column to an SFrame\n",
    "    # add the column 'constant' to the front of the features list so that we can extract it along with the others:\n",
    "    features = ['constant'] + features # this is how you combine two lists\n",
    "    # select the columns of data_SFrame given by the features list into the SFrame features_sframe (now including constant):\n",
    "    features_sframe = data_sframe[features]\n",
    "    # the following line will convert the features_SFrame into a numpy matrix:\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    # assign the column of data_sframe associated with the output to the SArray output_sarray\n",
    "    output_sarray = data_sframe[output]\n",
    "    # the following will convert the SArray into a numpy array by first converting it to a list\n",
    "    output_array = output_sarray.to_numpy()\n",
    "    return(feature_matrix, output_array)\n",
    "\n",
    "def predict_output(feature_matrix, weights):\n",
    "    return np.dot(feature_matrix, weights)\n",
    "\n",
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price')\n",
    "\n",
    "my_weights = np.array([1., 1.]) # the example weights\n",
    "my_features = example_features[0,] # we'll use the first data point\n",
    "predicted_value = np.dot(my_features, my_weights)\n",
    "print 'predicted value ' + str(predicted_value)\n",
    "\n",
    "test_predictions = predict_output(example_features, my_weights)\n",
    "print test_predictions[0] # should be 1181.0\n",
    "print test_predictions[1] # should be 2571.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    # Assume that errors and feature are both numpy arrays of the same length (number of data points)\n",
    "    # compute twice the dot product of these vectors as 'derivative' and return the value\n",
    "    derivative = 2 * np.dot(errors, feature)\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23345850022.0\n",
      "-23345850022.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \n",
    "my_weights = np.array([0., 0.]) # this makes all the predictions 0\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "# just like SFrames 2 numpy arrays can be elementwise subtracted with '-': \n",
    "errors = test_predictions - example_output # prediction errors in this case is just the -example_output\n",
    "feature = example_features[:,0] # let's compute the derivative with respect to 'constant', the \":\" indicates \"all rows\"\n",
    "derivative = feature_derivative(errors, feature)\n",
    "print derivative\n",
    "print -np.sum(example_output)*2 # should be the same as derivative"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we've created all of our functions we need and computed our derivitives, its time to implement our gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt \n",
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    while not converged:\n",
    "        # compute the predictions based on feature_matrix and weights using your predict_output() function\n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "        # compute the errors as predictions - output\n",
    "        errors = predictions - output\n",
    "        gradient_sum_squares = 0 # initialize the gradient sum of squares\n",
    "        # while we haven't reached the tolerance yet, update each feature's weight\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n",
    "            # compute the derivative for weight[i]:\n",
    "            derivative = feature_derivative(errors, feature_matrix[:,i])\n",
    "            # add the squared value of the derivative to the gradient magnitude (for assessing convergence)\n",
    "            gradient_sum_squares = gradient_sum_squares + (derivative * derivative)\n",
    "            # subtract the step size times the derivative from the current weight\n",
    "            weights[i] = weights[i] - (step_size * derivative)\n",
    "        # compute the square-root of the gradient sum of squares to get the gradient matnigude:\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A few things to note before we run the gradient descent. Since the gradient is a sum over all the data points and involves a product of an error and a feature the gradient itself will be very large since the features are large and the output is large. So while you might expect tolerance to be small, small is only relative to the size of the features. \n",
    "\\\\ \\\\\n",
    "Although the gradient descent is designed for multiple regression since the constant is now a feature we can use the gradient descent function to estimat the parameters in the simple regression on squarefeet. The folowing cell sets up the feature\\_matrix, output, initial weights and step size for the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-46999.88716555    281.91211912]\n",
      "281.9\n"
     ]
    }
   ],
   "source": [
    "# let's test out the gradient descent\n",
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'\n",
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n",
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "tolerance = 2.5e7\n",
    "\n",
    "simple_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)\n",
    "print simple_weights\n",
    "print round(simple_weights[1], 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One thing we can notice about this right away is that the value estimated in our simple linear regression was also 281. So this shows that with one feature, we are doing simple linear regression through gradient decent. This shows that our gradient decent algorithm is doing its job. From here, we need to use our test data to see how this approximates our new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356134.0\n"
     ]
    }
   ],
   "source": [
    "(test_simple_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)\n",
    "simple_predictions = predict_output(test_simple_feature_matrix, simple_weights) #compute predictions\n",
    "print round(simple_predictions[0]) #nearest dollar prediction of the first house in the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This number is the value predicted by the simple prediction model for the value of the first house in our data set. Now we will make a prediction on a more complex model and then look at what the actual price was for that data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -9.99999688e+04   2.45072603e+02   6.52795277e+01]\n"
     ]
    }
   ],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15'] \n",
    "# sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "initial_weights = np.array([-100000., 1., 1.])\n",
    "step_size = 4e-12\n",
    "tolerance = 1e9\n",
    "\n",
    "multiple_weights = regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance)\n",
    "print multiple_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366651.0\n"
     ]
    }
   ],
   "source": [
    "(test_multiple_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)\n",
    "multiple_predictions = predict_output(test_multiple_feature_matrix, multiple_weights)\n",
    "print round(multiple_predictions[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This model predicted the house to be worth \\$366,651 while our simple model predicted the value \\$356,134. Thats a \\$10,000 difference. Which model is closest to the actual price this house sold for? Lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310000.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['price'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can see that both of our models overestimated the value of the house by quite a bit. In fact our more complex model did even worse predicting the sell price than the one that only looked at square footage. Does this means that model did worse overall? Lets take a look at the RSS for the data set trained on our complex model and that will tell us if the complex model did worse than the simple model overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.70263446465e+14\n",
      "2.75400047593e+14\n",
      "2.70263446465e+14(RSSm) > 2.75400047593e+14(RSSs)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "multiple_test_errors = multiple_predictions - test_output\n",
    "RSSm = sum(multiple_test_errors * multiple_test_errors)\n",
    "\n",
    "test_errors = simple_predictions - test_output\n",
    "RSSs = sum(test_errors * test_errors)\n",
    "\n",
    "print str(RSSm)+'(RSSm) > ' + str(RSSs) + '(RSSs)'\n",
    "print RSSm > RSSs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can conclude that even though our complex model did a worse job of predicting the house price on the first data point, it did a better job of predicting the test data overall. As with any statistical analysis there are going to be outliers in the data that aren't going to reflect the data as a whole. Clearly in this case, the complex model did a better job of predicting than the simple model did.\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\section{Assessing Performance}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\subsection{\\emph{Measuring loss}}\n",
    "\\begin{center}\n",
    "\\emph{''Remember that all models are wrong; \\\\ \n",
    "The practical question is how wrong do they have to be to not useful?'' \\\\}\n",
    "-George Box, 1987\n",
    "\\end{center}\n",
    "\n",
    "The quote from statistician George Box points out that no model will be perfect to true life. The point of a model is to approximate as close as we can to make our model useful for our purposes. The goal of this section is to assess the quality of our model and to find metrics to show us how well it approximates real life. We will also look at some techniques to improve the performance of our models. \n",
    "\\\\ \\\\\n",
    "It was mentioned earlier that we've been treating loss symmetrically. In other words we've been treating the cost of over estimating the same as under estimating and this doesn't fit our model very well because the consiquenses of the two types of loss are not the same. Predicting a house price that's too high will result in less people looking and low to no offers. Whereas an underestimate will likely result in monitary loss for the seller.\\\\\n",
    "In a hypothetical perfrect model, the value of our loss will be zero, because its perfect. Therefore we need a metric to measure the loss in a real world case. This metric is known as a \\emph{Loss Function}: \\\\\n",
    "\\begin{eqnarray}\n",
    "&L(y, \\hat{f}(x)) \\\\\n",
    "&| y - \\hat{f}(x) | \\\\\n",
    "&(y - \\hat{f}(x))^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "Equation (17) above is our loss function. What our loss function is telling us is $L$ is the cost of using $\\hat{f}$ (our predicted value) at $x$ when $y$ is true. $y$ is also refered to as the true value or the truth. \\\\\n",
    "Equation (18) is known as the absolute error and equation (19) is the squared error. Both of these error functions in (18) and (19) are still assuming that the loss for underprediction is symmetric to overpredition. \\\\ \\\\\n",
    "\n",
    "\\subsection{\\emph{Training Error}}\n",
    "\n",
    "When we're creating and assessing a model we're limited to the data avalible to us. Its unlikely that the data will have every single possible data point possible contained within it. That's why in our earlier examples we broke our data set into training and testing. This simulates having new unseen real world data to introduce to the model. \\\\\n",
    "When looking at training error, realize that when we put values into our loss function, its assuming that the data points are all there is. This means as we train our model our training error will continue to decrease. \\\\\n",
    "The training error itself is just the average loss on all points in the dataset. More formally:\n",
    "\\begin{equation}\n",
    "\\frac{1}{N} \\sum_{i=1}^{N} L(y_i , \\hat{f}(x_i))\n",
    "\\end{equation}\n",
    "This is just simply the average of the sum of the loss function. For the loss function squared error from equation (19):\n",
    "\\begin{equation}\n",
    "\\frac{1}{N} \\sum_{i=1}^{N} L(y_i - \\hat{f}(x_i))^2\n",
    "\\end{equation}\n",
    "\n",
    "When we start looking at code, the \\emph{Root Mean Squared Error} (RMSE) is used to control the growth of the functional values.\n",
    "\\subsubsection{Training Error vs. Model Complexity}\n",
    "Model complexity and training error have a very intuitive relationship. In the most simple possible model complexity we will just have a constant horizonal line. This will have the highest model in most cases because there will be very few data points that fit the model unless they have a constant relationship. To put it in context of our housing example it would be like saying that no matter the feature x, wheather its square feet or number of bathrooms has no effect on price. This may be the kind of model you would see if you were predicting housing prices based on a feature that had absolutely no impact on the sales price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f46144168d0>,\n",
       " <matplotlib.lines.Line2D at 0x7f45c7bac050>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEKCAYAAAA7LB+5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbWaWOAe1mVniHNRmZolzUJuZJe7/AeuJo7RDCnByAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45c7c5f090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title('Simplest Model')\n",
    "plt.ylim(0,9)\n",
    "plt.xlim(0,9)\n",
    "line = list()\n",
    "for i in range(10):\n",
    "    line.append(4.3)\n",
    "\n",
    "x = np.array([1, 2, 2.5, 3, 4.25, 5.5, 6, 7.15, 7.75])\n",
    "y = np.array([1, 2.75, 2, 4, 4.5, 5, 7, 8, 8.5])\n",
    "\n",
    "plt.plot(x,y,'o',line, '-')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we can see from the graph above, it does an okay job predicting those few points in the middle, however the error between the points on either end are extreme. So in our relationship of error vs. model complexity the error is very large when the complexity is low. Lets visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f45c79d2ad0>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEZCAYAAABy91VnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45c7d22350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt1 = plt\n",
    "plt1.xlim(0,9)\n",
    "plt1.ylim(0,9)\n",
    "plt1.title(\"Training Error vs. Model Complexity\")\n",
    "plt1.xlabel('Model Complexity')\n",
    "plt1.ylabel(\"Error\")\n",
    "simplest_model_x = 1\n",
    "simplest_model_y = 8.75\n",
    "plt1.annotate('Simplest Model', xy=(1.25, 8.65), xytext=(2, 8),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            )\n",
    "plt1.plot(simplest_model_x, simplest_model_y, 'o')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now lets take a look at a function like what we discovered in our simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f45c799e790>,\n",
       " <matplotlib.lines.Line2D at 0x7f45c7850fd0>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEKCAYAAAA7LB+5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45e86515d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Linear Model')\n",
    "plt.ylim(0,9)\n",
    "plt.xlim(0,9)\n",
    "line = list()\n",
    "for i in range(10):\n",
    "    line.append(i)\n",
    "\n",
    "x = np.array([1, 2, 2.5, 3, 4.25, 5.5, 6, 7.15, 7.75])\n",
    "y = np.array([1, 2.75, 2, 4, 4.5, 5, 7, 8, 8.5])\n",
    "\n",
    "plt.plot(x,y,'o',line, '-')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Clearly, this increase in the model complexity puts our prediction points very close to our line, which means that our training error decreased. So when we add this to our comparison chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f45c70c18d0>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEZCAYAAABy91VnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45c75f5a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlim(0,9)\n",
    "plt.ylim(0,9)\n",
    "plt.title(\"Training Error vs. Model Complexity\")\n",
    "plt.xlabel('Model Complexity')\n",
    "plt.ylabel(\"Error\")\n",
    "x = np.array([1, 2])\n",
    "y = np.array([8.75, 7.5])\n",
    "plt.annotate('Linear Model', xy=(2.25, 7.5), xytext=(3, 7.45),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            )\n",
    "plt.plot(x, y, 'o')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As the model increases in flexibility, we can see the better its going to fit the training data. we can see an example of this as we get into a more complex model like we would have seen in our polynomial regression. \\\\ \\\\ \n",
    "\n",
    "Now we can look at a function that has more flexibility to fit the model and therefore a more complex function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f45c66e3210>,\n",
       " <matplotlib.lines.Line2D at 0x7f45c66e3310>,\n",
       " <matplotlib.lines.Line2D at 0x7f45c66e3a90>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmATx/4ZoyRFWb2J5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45c6940610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(t):\n",
    "    return np.exp(-t) * np.cos(2*np.pi*t)\n",
    "\n",
    "t1 = np.arange(0.0, 5.0, 0.1)\n",
    "plt.plot(t1, f(t1), 'bo', t2, f(t2), 'k', [1,2,3],[-0.2, 0.6, -0.2], 'rx')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This looks like a near perfect representation of the model to data. The issue is that in real world situations, the training set will likely not contain anywhere close to every possible observable data point. If we look closely at the above graph, the red x's represent test or real world data points. As we can see, while the model represents the training data almost exactly, when we give it new values to predict, it performs badly on any data outside the training set. This is what is known as \\emph{overfitting} a model. We must find the balance of the function to the training data that will give us the lowest error when we run that model against data outside of the training set.\\\\ \\\\ \n",
    "\n",
    "Now when we add our complex model to our model complexity vs. training error we see the relationship between our model complexity and training error more clearly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f45c6240b10>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEZCAYAAABy91VnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWQVzkjczq2BO8mZmFez/ANwKRGipQhumAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45c651c350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlim(0,9)\n",
    "plt.ylim(0,9)\n",
    "plt.title(\"Training Error vs. Model Complexity\")\n",
    "plt.xlabel('Model Complexity')\n",
    "plt.ylabel(\"Error\")\n",
    "x = np.array([1, 2, 8])\n",
    "y = np.array([8.75, 7.5, 0.5])\n",
    "plt.annotate('Complex Model', xy=(7.75, .75), xytext=(5\n",
    "                                                 , 1.5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            )\n",
    "plt.plot(x, y, 'o')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If we were to compare the same information of the test error vs model complexity we would expect to see more of a porabola-shaped curve rather than a exponetially shrinking one. This is intuitive given the information presented above. Clearly, in most cases, the extremely simple constant output value would give a very lousy prediction, so just like in our training model vs complexity chart we would see our first point in the upper left corner of the graph beacuse it would be a very simple model with very high error. As the complexity of the function increased we would start to see the error decreasing until the model started to get overfit to the data, at which time the error would beging to increase as the complexity increased. So there will be a low point at which we will have minimized the error with a certain model complexity. This is the optimal complexity for our model. \\newpage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\section{Regularization}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\subsection{\\emph{Ridge Regression}}\n",
    "\n",
    "One major roadblock to creating complex models is the risk of overfitting when a model is using too many features. The way this is accompished is by adding a term known as \\emph{cost-of-fit} to preder smaller coefficients. The cost is defined as \\emph{measure of fit + measure of magnitude of coefficients}. When the measure of fit metric is small it means that it is a good fit to the data. In contrast the measure of magnitude of the coefficent defines the overfit metric of the model so the smaller this value the less likely it is to be overfit. The balance of these two terms is going to give us the lowest total cost. We can use our RSS as our measure of fit, since both of them represent a good fit to the training data as the value of the metric goes down. We also know from RSS, that the lower this metric doesn't always translate into a good prediction. This is when we balance this out with the measure of the magnitude of regression coefficient. There are a couple different ways to look at calculating the magnitude of a function's coefficents. The most intuitive way would just be to sum the coefficents but this gives some issues because if there are some really large positive and some really large negitive coefficents you can end up with a small magnitide of coefficents which doesnt give us a good representation. The next intuitive way of dealing with the issue with the sum, is to sum the absolute values of the coefficents. This is known as $L_1$ norm, or lasso regression and will be covered in the next section. The other way to normalize the magnitude is to take the sum of the squares, again similar to the RSS. This is known as $L_2$ or ridge regression. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
