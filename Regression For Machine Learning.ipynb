{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{center}\n",
    "\\author{Kyle Hewitt}\n",
    "\\end{center}\n",
    "\\pagenumbering{roman}\n",
    "\\newpage\n",
    "\n",
    "\\section{Introduction}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\pagenumbering{arabic}\n",
    "\n",
    "Regression Machine Learning is a way to create predictive models from exsisting data to predict outcomes of new, never before seen data. This style of machine learning uses exsisting data to train and access the quality of the predicted model by splitting the exsisting data into training, validation and testing sets. As we will see when exploring farther into the different regression models, it is very easy to overfit a model to the training data so that the model looks like its perfect for the information its given but when new data is introducted, the predictions can be wildly wrong and sparatic. The idea of Regression models is to use the training data to create a model and then use the validation and testing sets to verify that these models are not overfit and how well they estimate on data outside the training set. There are different error metrics used throughout to test this idea of how well the model fits such as Residual Sum of Squares and RSME. In our experimentation we will be using the gradiant decent and coodinate decent algorithms in order to find the optimal model parameters, but it is worth mentioning that most of the problems addressed can be solved in closed form as well. Techniques such as Linear Regression, Regularization through Ridge and Lasso Regression, and Nearest Neighbor/Kernal Regression will be addressed.\n",
    "\\\\ \\\\\n",
    "\\texttt{{\\small All the code presented here can be referenced at https://github.com/redonelima/regression}}\n",
    "\\\\ \\\\\n",
    "\\subsection{\\emph{Notation and software packages used}}\n",
    "\\subsubsection{Software Packages Used}\n",
    "Throughout there will usually be two implementations of each idea, one that uses GraphLab and SFrame and another that will be implemented myself. What to expect from this is to see the behavior with these exsisting tools and then xperimentation in self implementation. Other tools can be used such as sypi and pandas, but this course uses GraphLab because it is good for large datasets because it doesn't require the entire data set to be in memory, instead it only pulls the data into memory as it is needed. This causes the import of GraphLab and the import of the Frame to take a while to load, however, once this is completed operations run faster and large-scale data sets can be used. In the example of the kc\\_house\\_data.gl set has 17,258 house entries, each with 21 features. \\\\ \\\\\n",
    "The Numpy library is used for complex matrix calculations. Numpy is a python library which is written in fortran for extremely efficent matrix calculations. These calculations could be done manually in python, but given large data sets, this would be very slow and inefficent.\n",
    "The MathPlotLib python library is used to visulize data throughout. Graphlab also has functions like .show() to visualize data. \\\\ \\\\\n",
    "The following import will be used throughout most of the code examples thoughout:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to khewitt08@live.com and will expire on December 28, 2018.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1515119263.log\n"
     ]
    }
   ],
   "source": [
    "import graphlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "graphlab.canvas.set_target('ipynb', port=None)\n",
    "sales = graphlab.SFrame('kc_house_data.gl/')\n",
    "train_data,test_data = sales.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "\n",
    "\\section{Regression}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\subsection{\\emph{What is Regression?}}\n",
    "From a high level perspective, regression is the way to learn about a relationship between \\textbf{\\textit{features}} and \\textbf{\\textit{predictions}}. Throughout this case study approtch we will be looking at the use-case of predicting housing prices using regressive learning but the scope of regression goes well beyond this, and many of the techniques used throughout the experiments have impacts on other areas of machine learning besides regression such as bias-varience tradeoff which will be covered in depth later on. In the context of regression, the \\textbf{\\textit{features}} of a model are considered the independant variables and those\n",
    "are derived from the data. For example, for our housing predictive case, the features could be squarefeet,\n",
    "number of bathrooms, lot size, ect. These independant variables are what determine the value of our\n",
    "dependant, \\textbf{\\textit{predictive}} values, which in our example would be the value or predicted selling price of the home. It is the responsibility of the regressive model to learn the relationship between the independant and\n",
    "dependant variables.\n",
    "\n",
    "\\newpage\n",
    "\n",
    "\\section{Linear Regression}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "\\subsection{\\emph{Simple Linear Regression}}\n",
    "The idea of simple linear regression is to fit a line to a dataset. In the simplest form, the regression line is simply a line and intercept. Even if the complexity of the model pushes into a higher order polynomial, the main problem is figuring out which line approximates the best prediction to the test data. In the examples, a dataset of house sales in Kings County Seattle and their features will be used. We start by splitting the dataset into a training set and a test set. We do this because we can fit our model to fit the data perfectly but then any variation in future predictions will be very far from the actual value and the housing price won't be well represented. Therefore we must have a portion of our data serperated from the training set to test the quality of our model. \\\\\n",
    "In a simple linear model, the obvoius equation of the model will be, \n",
    "\\begin{equation} \n",
    "f(x) = w_0 + w_1 x\n",
    "\\end{equation}\n",
    "where the equation for the true point can be represented by the equation,\n",
    "\\begin{equation}\n",
    "y_i = w_0 + w_1 x_i + \\epsilon\n",
    "\\end{equation}\n",
    "where $\\epsilon$ represents the noise that causes variation from the model. This noise will be covered more in depth in the section covering performance assessments. \\\\\n",
    "The next point to address is to figure out how to form a metric to assess the quality of the model. The first way we will address this is called the Residual Sum of Squares (RSS), which is the squared sum of the difference between the real data point and the estimated point given by the prediction model.\n",
    "\\begin{equation}\n",
    "RSS(w_0, w_1) = \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\n",
    "\\end{equation}\n",
    "\\centerline{Where $\\hat{y_i} = w_0 + w_1 x_i$ is the value predicted from the model}\n",
    "\\\\ \\\\\n",
    "In the housing example, the most obvious feature to use would be the square footage of the house, represented by the column 'sqft living' in the dataset. In the simple slope-intercept form, the $w_1$ slope term represents the change in price of the house for every unit of squarefeet. \n",
    "\n",
    "\n",
    "\\subsection{\\emph{Gradient Descent}}\n",
    "So the next step in finding the lowest error is minimizing the RSS. To do this we have to realize that a plot of every possible fit line will lead to a convex with the optimal point at the minimum. The gradient of a function is the derivitive vector in the increasing direction. So in order to find the minumum would be to subtract the gradient. This is known as the gradient decent algorthim. Given some function $g(w)$, when we take the derivitive with respect to $w$, then when $\\frac{dg(w)}{w}$ is negitive, it means we need to increase $w$ to reach the minimum and when the sign switches it means we need to decrease $w$.\\\\\n",
    "This algorithm, known as the gradient decent algorithm, starts at a point and takes steps in the opposite direction of the gradiant. This step size, denoted as $\\eta$ controls how far each iteration of the gradient decent steps towards the minimum. In the case where the derivitive of $g(x)$ is large, then the steps could be very large and may jump back and forth across the minimum and take a very long time to converge. On the other end, if $\\eta$ is too small, then it may take a very long time to converge. This means that step size must be choosen carefully.\\\\\n",
    "There are two popular choices in picking a step size.\n",
    "\\begin{center}\n",
    "\\begin{enumerate}\n",
    "\\item Fixed step size \\\\\n",
    "$\\eta = 0.1$\n",
    "\\item Decreasing step size \\\\\n",
    "$\\eta_t = \\frac{\\alpha}{t}$ \\\\\n",
    "$\\eta_t = \\frac{\\alpha}{\\sqrt{t}}$\n",
    "\\end{enumerate}\n",
    "\\end{center}\n",
    "\n",
    "\\noindent In a closed form solution, we would know that we would reach convergence when $\\frac{dg(w)}{dw}$ is zero. Since in a real world situation, with our step size $\\eta$, it could be impossible to reach zero within a reasonable amount of time. So we have to have looser convergence criteria. Therefore, a threshold criteria $\\varepsilon$ has to be established so that when $\\frac{dg(w)}{dw} < \\varepsilon$ we can say we're close enough within convergence to say that we can consider it convered. \\\\ \\\\\n",
    "\n",
    "\\noindent This gradient decent algorithm should look like: \\\\\n",
    "\\\\\n",
    "\\centerline{while $\\frac{dg(w)}{dw} \\geq \\varepsilon $:} \\\\\n",
    "\\centerline{\\indent $w^{(t+1)} = w^t - \\eta \\frac{dg(w^t)}{dw}$}\n",
    "\\\\ \\\\\n",
    "The final part of the gradient decent algorithm for our linear regression model is finding $\\frac{dg(w)}{dw}$ where $g(w_i) =  (y_i - w_0 + w_1 x_i)^2$ and since the sum of the derivitive is the derivitive of the sum we can take the derivitive of a single value with respect to $w_0$ and $w_1$ to create the gradient vector.\n",
    "\\begin{equation}\n",
    "\\bigtriangledown RSS(w_0, w_1) = \\begin{vmatrix} -2\\sum_{i=1}^{N} [y_i - \\hat{y}_i] \\\\ \\\\ -2\\sum_{i=1}^{N} [y_i - \\hat{y}_i] x_i \\end{vmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "This ends up being a vector because even in our linear equation we have two variables. This will be a very important concept when we look at multiple regression in the next section where we start adding other features like number of bathrooms or bedrooms ontop of square footage. Given this vector, we can used a closed form solution to solve for each of the unknowns in closed form:\n",
    "\\begin{equation}\n",
    "\\hat{w}_0 = \\frac{\\sum y_i}{N} - \\hat{w}_1 \\frac{\\sum x_i}{N}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\hat{w}_1 = \\frac{\\sum y_i x_i - \\frac{\\sum y_i x_i}{N}}{\\sum x_i ^2- \\frac{\\sum x_i \\sum x_i}{N}}\n",
    "\\end{equation}\n",
    "\n",
    "Now that we have these closed form solutions for these two variables. There's a lot of similarities between these two equations so we can pull out a few main calculations that we need, namely:\n",
    "\\\\ \\\\\n",
    "\\indent \\indent \\indent \\indent \\indent \\indent \\indent\n",
    "\\indent 1. $\\sum y_i$\n",
    "\\indent 2. $\\sum x_i$\n",
    "\\indent 3. $\\sum y_i x_i$\n",
    "\\indent 4. $\\sum x_i^2$ \n",
    "\\\\ \\\\\n",
    "Using these, we can create a function for calculating simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_linear_regression(input_feature, output):\n",
    "    # compute the sum of input_feature and output\n",
    "    feature_sum = input_feature.sum()\n",
    "    output_sum = output.sum()\n",
    "    num_inputs = output.size()\n",
    "    \n",
    "    # compute the product of the output and the input_feature and its sum\n",
    "    product_sum = (input_feature *output).sum()\n",
    "    \n",
    "    # compute the squared value of the input_feature and its sum\n",
    "    squared_sum = (input_feature ** 2).sum()\n",
    "    # use the formula for the slope\n",
    "    numerator = product_sum - ((float(1)/num_inputs) * (feature_sum*output_sum))\n",
    "    denominator = squared_sum - ((float(1)/num_inputs) * (feature_sum*feature_sum))\n",
    "    slope = numerator/denominator\n",
    "    # use the formula for the intercept\n",
    "    intercept = output.mean() - slope * input_feature.mean()\n",
    "    return (intercept, slope)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we can test this function by giving it input that we know the outcome. \\\\\n",
    "We are going to make a feature and then put the output exactly on a line. This results in both our slope and interscept being 1. It will also be used to test our model since all the points will fall on a line it means out RSS will be exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 1.0\n",
      "Slope: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_feature = graphlab.SArray(range(5))\n",
    "test_output = graphlab.SArray(1 + 1*test_feature)\n",
    "(test_intercept, test_slope) =  simple_linear_regression(\n",
    "    test_feature, test_output)\n",
    "print \"Intercept: \" + str(test_intercept)\n",
    "print \"Slope: \" + str(test_slope)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As expected, we see that both slope and intercept of our simple test case are 1. \\\\ Next, its time to use our simple linear regression model on some actual data. As mentioned earlier our feature for the focus of this model is going to be the square footage of the house and our predictive $\\hat{y}$ is going to be the sales price of the house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -47116.0765749\n",
      "Slope: 281.958838568\n"
     ]
    }
   ],
   "source": [
    "sqft_intercept, sqft_slope = simple_linear_regression(\n",
    "    train_data['sqft_living'], train_data['price'])\n",
    "\n",
    "print \"Intercept: \" + str(sqft_intercept)\n",
    "print \"Slope: \" + str(sqft_slope)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the above code we can see that we have an intercept approximately $-47,116$ and a slope of approximately $282$. \\\\\n",
    "What this means for us is that a house with no square feet (an empty lot) would actually cost the seller $\\$ 47,000$ for someone to take. Clearly, this intercept has very little significance for our purposes.\n",
    "\\\\\n",
    "However, the slope value of 281.96 is significant and it means that for every square foot extra a house has, the sales price of the house increases by about $\\$ 282$. Clearly this is not the most accurate model, since there are many other factors that can play into the value of a home, but that's the reason we're calling this a 'simple' model. \n",
    "\\\\ \\\\\n",
    "Now that we have a model, we can start making predictions.\n",
    "\\subsection{Code Example}\n",
    "\\rule{\\textwidth}{2pt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_regression_predictions(input_feature, intercept, slope):\n",
    "    # calculate the predicted values:\n",
    "    return  slope*input_feature + intercept"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have our model and our prediction function we can now use it to predict new houses based on the square footage feature. Lets look at a house that has a size of 2650 sqft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated price for a house with 2650 squarefeet is $700074.85\n"
     ]
    }
   ],
   "source": [
    "my_house_sqft = 2650\n",
    "estimated_price = get_regression_predictions(\n",
    "    my_house_sqft, sqft_intercept, sqft_slope)\n",
    "print \"The estimated price for a house with %d squarefeet is $%.2f\" % (\n",
    "    my_house_sqft, estimated_price)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we can see, using our predictive model a house with 2650 sqft is estimated to be sold for \\$ 700,000. \\\\ \\\\\n",
    "Lets take a look at the quality of our model. We will calculate our RSS, which we already know should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_residual_sum_of_squares(input_feature, output, intercept, slope):\n",
    "    # First get the predictions\n",
    "    predicted_price = get_regression_predictions(input_feature, intercept,slope)\n",
    "    # then compute the residuals (since we are squaring it doesn't matter which order you subtract)\n",
    "    residual = output - (slope*input_feature+intercept)\n",
    "    # square the residuals and add them up\n",
    "    RSS = residual * residual\n",
    "    return(RSS.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print get_residual_sum_of_squares(\n",
    "    test_feature, test_output, test_intercept, test_slope) \n",
    "# should be 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RSS of predicting Prices based on Square Feet is : 1.20191835632e+15\n"
     ]
    }
   ],
   "source": [
    "rss_prices_on_sqft = get_residual_sum_of_squares(\n",
    "    train_data['sqft_living'], train_data['price'], \n",
    "    sqft_intercept, sqft_slope)\n",
    "print 'The RSS of predicting Prices based on Square Feet is : ' + str(\n",
    "    rss_prices_on_sqft)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can use this to go the other way as well. Given a price, we can tell how big of a house that a buyer can afford."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_regression_predictions(output, intercept, slope):\n",
    "    return  (output-intercept)/slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated squarefeet for a house worth $800000.00 is 3004\n"
     ]
    }
   ],
   "source": [
    "my_house_price = 800000\n",
    "estimated_squarefeet = inverse_regression_predictions(\n",
    "    my_house_price, sqft_intercept, sqft_slope)\n",
    "print \"The estimated squarefeet for a house worth $%.2f is %d\" % (\n",
    "    my_house_price, estimated_squarefeet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So we can use this model to say that a buyer with $\\$ 800,000$ could afford a house up to $3,004 sqft$. \n",
    "\\\\ \\\\\n",
    "Now that we have a simple linear model on our housing data, we can change up the features and see how it fits a model when looking at bedrooms and see how that compares to the prediction of square footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 109473.180469\n",
      "Slope: 127588.952175\n"
     ]
    }
   ],
   "source": [
    "bedrooms_intercept, bedrooms_slope = simple_linear_regression(\n",
    "    train_data['bedrooms'], train_data['price'])\n",
    "\n",
    "print \"Intercept: \" + str(bedrooms_intercept)\n",
    "print \"Slope: \" + str(bedrooms_slope)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So based on our bedroom model, we can see that we get an increase in housing price of \\$ 127,589 per bedroom. So now that we have our two training models, its time to see how well these predict our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RSS of predicting Prices based on Bedrooms is : 4.93364582868e+14\n"
     ]
    }
   ],
   "source": [
    "# Compute RSS when using bedrooms on TEST data:\n",
    "rss_prices_on_bedrooms_test = get_residual_sum_of_squares(\n",
    "    test_data['bedrooms'], test_data['price'],\n",
    "    bedrooms_intercept, bedrooms_slope)\n",
    "print 'The RSS of predicting Prices based on Bedrooms is : ' + str(\n",
    "    rss_prices_on_bedrooms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RSS of predicting Prices based on Square Feet is : 2.75402936247e+14\n"
     ]
    }
   ],
   "source": [
    "# Compute RSS when using squarfeet on TEST data:\n",
    "rss_prices_on_sqft_test = get_residual_sum_of_squares(\n",
    "    test_data['sqft_living'], test_data['price'], \n",
    "    sqft_intercept, sqft_slope)\n",
    "print 'The RSS of predicting Prices based on Square Feet is : ' + str(\n",
    "    rss_prices_on_sqft_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have our RSS of our squarefoot model and our bedroom model, we can comare our two RSS and see that since the squarefootage model has a lower RSS on the testing data it does a better job of a predictive model. This is because we trained our model on the training data, so all of the houses in the testing set are all houses that the model has never seen. Since the RSS of squarefeet is lower, it means that the houses in the testing with the squarefeet feature set were all closer to the prediction model than those of the bedroom feature set.\n",
    "\\\\ \\\\\n",
    "It is important at this point to address the fact that this model is assuming an asyemetric error, meaning that we're assuming the same consiquence for under estimating than over estimating. With housing, this may not be the case. A house whos value is over estimated may not get many offers and therefore take longer to sell. A house that is underestimated may listed too low and the buyer ends up losing money. The consequences of each of these errors are actually asymmetric, but this adds an entirely new level of complexity so we will be treating errors symmetricly. \n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\section{Multiple Regression}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "Anyone who has ever had to deal with valueing a home knows there's a lot more to it than just the square footage or the number of bedrooms by themselves. In this section we will be looking at multiple regression, meaning that we're going to be building a regression model based off of multiple features and not just off a single one. We will also be looking at what is known as polynomial regression which will allow us to fit our model to a higher order polynomial, giving our model much more flexibility. \\\\\n",
    "\\\\\n",
    "There are not always exponential relationships between features and their trends. An example of a exponential polynomial equation could be $y_i = w_0 + w_2 t_i + w_3 t_i^2 + ... + w_n t_i^n$ where as a more complex model that has a linearly upward trend could look like $y_i = w_0 + w_1 t_i + w_2 \\sin{( \\frac{2 \\pi t_i}{12} - \\phi )} + \\epsilon _i$ \\\\\n",
    "This second equation represents seasonality in a linearly upward trend, which is a good representitive model of anything that has variying trends throughout the year but has an overall upward trend. Examples of this are camping equipemnt sales that have higher sales thoughout the summer during the camping season. Clearly this will give an overall better prediction than a linear model, but this is much harder to model. This is where the power of regressive learning and gradient decent start to really show their power.\n",
    "\\subsection{\\emph{The Polynomial Regression Model}}\n",
    "We are going to look at this in general form, also known as the generic basis expansion. Each $j^{th}$ feature will have a bias, as seen with the example of the $\\sin$ function above, which we will refer to as $h_j (x_i)$ which is the $j^{th}$ feature of data input i refered to as $x_i$.\n",
    "\\\\\n",
    "So the generic basis expansion for each $x_i$ data input:\n",
    "\\begin{equation}\n",
    "\\sum_{j=0}^{D} w_j h_j (x_i) + \\epsilon_i\n",
    "\\end{equation}\n",
    "\\centerline{Where D is the number of features for each data point}\n",
    "This means that instead of being in a 2 dementional space like we were when we were looking at a single feature, we will be looking at a D+1 dimentional space when looking at multiple features. So for a model that accounts for 2 features, we'd be looking at a hyperplane in a 3D space instead of a line in a 2D space. This is also known as a D-dimensional curve. \\\\ \\\\\n",
    "So how do we look at this regression model in this new D-dimentional space? We have to start looking at the data points in matrix notation since we're dealing with an $n \\times m$ matrix of observations which we will refer to as $\\textbf{H}$ where $n$ is the number of features $D$ and $m$ is the number of observations $i$. \\\\ $\\textbf{H}$ is the multiplied by the $D \\times 1$ feature vector $\\vec{w}$. Finally we add in our error vector $\\vec{\\epsilon}$. Finally this will give us our final vector $\\vec{y}$.\n",
    "\\begin{equation}\n",
    "\\vec{y} = \\textbf{H} \\vec{w} + \\vec{\\epsilon}\n",
    "\\end{equation}\n",
    "We will start by looking at an instance of $D$ observations, each with a single feature, like in our linear regression model. because of the rules of vector multiplcation, we will take the transpose of this observation/feature vector, denoted $\\vec{h}$. This is then multiplied by the coefficent vector $\\vec{w}$ and added to our single $\\epsilon$ value. \n",
    "\\begin{equation}\n",
    "y_i = \\vec{h}^T(x_i) \\vec{w} + \\epsilon\n",
    "\\end{equation}\n",
    "So our vectors for a single iteration would look like:\n",
    "\\begin{equation}\n",
    "y_i = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "w_0 & w_1 & w_2 & \\ldots & w_D\n",
    "\\end{array}\n",
    "\\right] + \\left[\n",
    "\\begin{array}{c}\n",
    "h_0(x_i) \\\\\n",
    "h_1(x_i) \\\\\n",
    "h_2(x_i) \\\\\n",
    "\\vdots \\\\\n",
    "h_D(x_i)\n",
    "\\end{array}\n",
    "\\right]\n",
    "+ [\\epsilon]\n",
    "\\end{equation}\n",
    "\n",
    "So lets look at this as a matrix of $N$ observations:\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_N\n",
    "\\end{array}\n",
    "\\right] = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "h_0(x_1) & h_1(x_1) & \\ldots & h_D(x_1) \\\\\n",
    "h_0(x_2) & h_1(x_2) & \\ldots & h_D(x_2) \\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots   \\\\\n",
    "h_0(x_N) & h_1(x_N) & \\ldots & h_D(x_N)\n",
    "\\end{array}\n",
    "\\right]  \\left[\n",
    "\\begin{array}{c}\n",
    "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D\n",
    "\\end{array}\n",
    "\\right] + \\left[\n",
    "\\begin{array}{c}\n",
    "\\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_N\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\subsection{\\emph{Computing the RSS}}\n",
    "Now that we have our multiple feature regression model, we need to figure out how we're going to assess the cost of a given model in order to find the one that best fits our predictive model. Just like we did in our simple linear model, we will use RSS as a metric for this cost. Just like with our model we will write the RSS in terms of a single component and then scale it up. Since we know that the RSS is the difference between the actual value and the predicted value, our first part of the RSS equation will remain the same. What will change is the definition of our $\\hat{y}$ value. We will write this in terms of the model itself this time instead of in terms of $\\hat{y}$ because it will be easier to scale it up into matrix form. \n",
    "\\begin{equation}\n",
    "RSS(\\vec{w}) = \\sum_{i=1}^{N} (y_i - \\vec{h}^T(x_i) \\vec{w})^2\n",
    "\\end{equation}\n",
    "Since we know that $\\vec{h}(x_i)$ is a single component of $\\textbf{H}$, then the matrix form of $\\vec{h}(x_i) \\vec{w}$ would be $\\textbf{H}\\vec{w}$ and to square each component in the resulting vector we would multiply the resulting vecor by its transpose. Therefore our matrix notation of the RSS would be:\n",
    "\\begin{equation}\n",
    "RSS(\\vec{w}) = \\sum_{i=1}^{N} (y - \\textbf{H}\\vec{w})^T (y - \\textbf{H}\\vec{w})\n",
    "\\end{equation}\n",
    "\n",
    "\\subsection{\\emph{Gradient of the RSS}}\n",
    "Now that we have our model for our RSS, we need to find the gradient in order to find our lowest cost model. The gradient is much more useful in this polynomial regression model than it did in the linear model because now we have more features. To find our gradient we will take the same steps as we did in earlier derivations and solve for our gradient in 1D and scale it up. To find the dirivitive of our 1D case we must note that both $y$, $h$ and $w$ are scalers in our 1D case.\n",
    "\\begin{eqnarray}\n",
    "\\frac{d}{dw} (y-hw)(y-hw) & = &\\frac{d}{dw} (y-hw)^2 \\\\\n",
    "                          & = &2(y-hw)^1 (-h) \\nonumber \\\\\n",
    "                          & = &-2h(y-hw) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\\centerline{So its not hard to see that when its scaled up our gradient is:}\n",
    "\\begin{eqnarray}\n",
    "\\bigtriangledown RSS(\\vec{w}) &= &\\bigtriangledown [(\\vec{y} - \\textbf{H}\\vec{w})^T (\\vec{y} - \\textbf{H}\\vec{w}) \\\\\n",
    "                              &= &-2\\textbf{H}^T(\\vec{y}-\\textbf{H}\\vec{w}) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\subsubsection{Closed Form Solution}\n",
    "Just like in our linear model, we can solve for our polynomial model in the same closed form way. Now that we have our gradient of our RSS, we can set it equal to zero and solve for $\\hat{w}$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "-2\\textbf{H}^T(\\vec{y}-\\textbf{H}\\vec{\\hat{w}}) &= &0 \\\\\n",
    "-\\not{2}\\textbf{H}^T\\vec{y} + \\not{2}\\textbf{H}^T \\textbf{H}\\vec{w}) &= &0 \\nonumber \\\\\n",
    "\\textbf{H}^T \\textbf{H}\\vec{\\hat{w}} &  = &\\textbf{H}^T\\vec{y} \\nonumber \\\\\n",
    "(\\textbf{H}^T\\textbf{H})^{-1} \\textbf{H}^T \\textbf{H}\\vec{\\hat{w}} & = &(\\textbf{H}^T\\textbf{H})^{-1} \\textbf{H}^T\\vec{y} \\nonumber \\\\\n",
    "\\textbf{I}\\vec{\\hat{w}} &= &(\\textbf{H}^T\\textbf{H})^{-1} \\textbf{H}^T\\vec{y} \\nonumber \\\\\n",
    "\\vec{\\hat{w}} &= &(\\textbf{H}^T\\textbf{H})^{-1} \\textbf{H}^T\\vec{y} \\nonumber \n",
    "\\end{eqnarray}\n",
    "\n",
    "This solved out very smoothly but there's a few problems when approching this in a closed form way like we did with the linear model. Both of the main issues have to do with the inverse. First, $(\\textbf{H}^T\\textbf{H})^{-1}$ is only invertable if, in most cases, the number of linearly independant observations N \\textgreater D. This limits the complexity of the models we can make. This issue is addressed later with regularization. The second issue is the complexity of doing the inverse. The complexity of inverting a $D \\times D$ matrix is $\\Omega(D^3)$ which can get very compulationaly expensive with large data sets. Therefore using this method is only an option on reasonably small sets of data. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\subsubsection{Gradient Descent}\n",
    "The gradient descent for polynomial regression isn't much different from linear regression, just accounting for more features. The first difference is the convergence criteria. Now that we have many other features accounted for in the gradient, now we must find the minimum of each of those features. The value cost of each feature is in the RSS vector. Therefore the convergence of the function would be when the magnitude of the gradient of the RSS is zero, or within the threshold $\\varepsilon$ in our real world case. More formally, $\\|\\bigtriangledown RSS(\\vec{w}^{(t)})\\|  \\textgreater  \\varepsilon$\n",
    "\\\\\n",
    "Then for each of the features, refered to as partials here, the sum of the dirivitive of the partials are multiplied by the step size $\\eta$ for each of the iterations $t$. \n",
    "\\\\ \\\\\n",
    "while $\\| \\bigtriangledown RSS(\\vec{w}^t) \\| \\textgreater \\varepsilon:$ \\\\\n",
    "\\indent for j in range(0, D): \\\\\n",
    "\\indent \\indent $partial_j = -2\\sum_{i=1}^{N} h_j(\\vec{x}_i)(y_i -                     \\hat{y}_i(\\vec{w}^{(t)}))$ \\\\\n",
    "\\indent \\indent$\\vec{w}_j^{(t+1)} = \\vec{w}_j^{t} - \\eta (partial_j)$ \\\\\n",
    "t++\n",
    "\\subsection{Code Example}\n",
    "\\rule{\\textwidth}{2pt}\n",
    "We will start out with using the GraphLab linear\\_regression model again. This time however we will be passing it a list of features which will cause it to run polynomial regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Linear regression:</pre>"
      ],
      "text/plain": [
       "Linear regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 17384</pre>"
      ],
      "text/plain": [
       "Number of examples          : 17384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of features          : 3</pre>"
      ],
      "text/plain": [
       "Number of features          : 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 3</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 4</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 1.068897     | 4146407.600631     | 258679.804477 |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 1.068897     | 4146407.600631     | 258679.804477 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_features = ['sqft_living', 'bedrooms', 'bathrooms']\n",
    "example_model = graphlab.linear_regression.create(\n",
    "    train_data, target = 'price', features = example_features, \n",
    "                                                  validation_set = None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We now have a trained model. From here we can look at the coefficents of the model and make predictions based off the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+---------------+\n",
      "|     name    | index |     value      |     stderr    |\n",
      "+-------------+-------+----------------+---------------+\n",
      "| (intercept) |  None | 87910.0724924  |  7873.3381434 |\n",
      "| sqft_living |  None | 315.403440552  | 3.45570032585 |\n",
      "|   bedrooms  |  None | -65080.2155528 | 2717.45685442 |\n",
      "|  bathrooms  |  None | 6944.02019265  | 3923.11493144 |\n",
      "+-------------+-------+----------------+---------------+\n",
      "[4 rows x 4 columns]\n",
      "\n",
      "271789.505878\n"
     ]
    }
   ],
   "source": [
    "example_weight_summary = example_model.get(\"coefficients\")\n",
    "print example_weight_summary\n",
    "example_predictions = example_model.predict(train_data)\n",
    "print example_predictions[0] # should be 271789.505878"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have a trained model we can compute the cost metric using RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_residual_sum_of_squares(model, data, outcome):\n",
    "    # First get the predictions\n",
    "    predictions = model.predict(data)\n",
    "    # Then compute the residuals/errors\n",
    "    errors = outcome - predictions\n",
    "    # Then square and add them up\n",
    "    RSS = (errors ** 2).sum()\n",
    "    return(RSS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7376153833e+14\n"
     ]
    }
   ],
   "source": [
    "rss_example_train = get_residual_sum_of_squares(example_model, test_data, test_data['price'])\n",
    "print rss_example_train # should be 2.7376153833e+14"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we've varified that our model is trained, we can add a few more featured based off of the features we already have. The reason for this is that features are somtimes codependant and more accurate models can be created by a combonation of other features. For this experiment we will be  the square of the bedroom count, the product of bedrooms and bathrooms, the log of the sqare feet, and the sum of the properties latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "train_data['bedrooms_squared'] = train_data['bedrooms'].apply(lambda x: x**2)\n",
    "train_data['bed_bath_rooms'] = train_data['bedrooms'] * train_data['bathrooms']\n",
    "train_data['log_sqft_living'] = train_data['sqft_living'].apply(lambda x: log(x))\n",
    "train_data['lat_plus_long'] = train_data['lat'] + train_data['long']\n",
    "\n",
    "test_data['bedrooms_squared'] = test_data['bedrooms'].apply(lambda x: x**2)\n",
    "test_data['bed_bath_rooms'] = test_data['bedrooms'] * test_data['bathrooms']\n",
    "test_data['log_sqft_living'] = test_data['sqft_living'].apply(lambda x: log(x))\n",
    "test_data['lat_plus_long'] = test_data['lat'] + test_data['long']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The purpose of squaring the bedrooms is to seperate data more between few bedrooms and many bedrooms. The product of the bedrooms and bathrooms is known as an \\emph{interaction} feature. It is large only when both bedrooms and bathroom counts are large. Taking the log of the square feet will spread out small values and bring larger values closer together. The final feature of the sum of the latitude and longitude is only to show that using poor feature selection leads to poor predictive models. \n",
    "\\\\\n",
    "For this experiment, we will make three seperate models. The first will have bedrooms, bathrooms, square feet, latitude, and laditude. All of these features exist in the orginal dataset.\\\\\n",
    "The next model will include all of the features of model 1 plus the remainder of the new features we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1_features = ['sqft_living', 'bedrooms', 'bathrooms', 'lat', 'long']\n",
    "model_2_features = model_1_features + ['bed_bath_rooms']\n",
    "model_3_features = model_2_features + ['bedrooms_squared', 'log_sqft_living', 'lat_plus_long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Linear regression:</pre>"
      ],
      "text/plain": [
       "Linear regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 17384</pre>"
      ],
      "text/plain": [
       "Number of examples          : 17384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of features          : 5</pre>"
      ],
      "text/plain": [
       "Number of features          : 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 5</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 6</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 0.098199     | 4074878.213096     | 236378.596455 |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 0.098199     | 4074878.213096     | 236378.596455 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Linear regression:</pre>"
      ],
      "text/plain": [
       "Linear regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 17384</pre>"
      ],
      "text/plain": [
       "Number of examples          : 17384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of features          : 6</pre>"
      ],
      "text/plain": [
       "Number of features          : 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 6</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 7</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 0.132620     | 4014170.932927     | 235190.935428 |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 0.132620     | 4014170.932927     | 235190.935428 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Linear regression:</pre>"
      ],
      "text/plain": [
       "Linear regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 17384</pre>"
      ],
      "text/plain": [
       "Number of examples          : 17384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of features          : 9</pre>"
      ],
      "text/plain": [
       "Number of features          : 9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 9</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 10</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 0.107355     | 3193229.177894     | 228200.043155 |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 0.107355     | 3193229.177894     | 228200.043155 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+---------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+---------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_1 = graphlab.linear_regression.create(train_data, target = 'price', \n",
    "                                            features = model_1_features, validation_set = None)\n",
    "model_2 = graphlab.linear_regression.create(train_data, target = 'price', \n",
    "                                            features = model_2_features, validation_set = None)\n",
    "model_3 = graphlab.linear_regression.create(train_data, target = 'price', \n",
    "                                            features = model_3_features, validation_set = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+---------------+\n",
      "|     name    | index |     value      |     stderr    |\n",
      "+-------------+-------+----------------+---------------+\n",
      "| (intercept) |  None | -56140675.7444 | 1649985.42028 |\n",
      "| sqft_living |  None | 310.263325778  | 3.18882960408 |\n",
      "|   bedrooms  |  None | -59577.1160682 | 2487.27977322 |\n",
      "|  bathrooms  |  None | 13811.8405418  | 3593.54213297 |\n",
      "|     lat     |  None | 629865.789485  | 13120.7100323 |\n",
      "|     long    |  None | -214790.285186 | 13284.2851607 |\n",
      "+-------------+-------+----------------+---------------+\n",
      "[6 rows x 4 columns]\n",
      "\n",
      "+----------------+-------+----------------+---------------+\n",
      "|      name      | index |     value      |     stderr    |\n",
      "+----------------+-------+----------------+---------------+\n",
      "|  (intercept)   |  None | -54410676.1152 | 1650405.16541 |\n",
      "|  sqft_living   |  None | 304.449298057  | 3.20217535637 |\n",
      "|    bedrooms    |  None | -116366.043231 | 4805.54966546 |\n",
      "|   bathrooms    |  None | -77972.3305135 | 7565.05991091 |\n",
      "|      lat       |  None | 625433.834953  | 13058.3530972 |\n",
      "|      long      |  None | -203958.60296  | 13268.1283711 |\n",
      "| bed_bath_rooms |  None | 26961.6249092  | 1956.36561555 |\n",
      "+----------------+-------+----------------+---------------+\n",
      "[7 rows x 4 columns]\n",
      "\n",
      "+------------------+-------+----------------+---------------+\n",
      "|       name       | index |     value      |     stderr    |\n",
      "+------------------+-------+----------------+---------------+\n",
      "|   (intercept)    |  None | -52974974.0602 |  1615194.9439 |\n",
      "|   sqft_living    |  None | 529.196420564  | 7.69913498511 |\n",
      "|     bedrooms     |  None | 28948.5277313  | 9395.72889106 |\n",
      "|    bathrooms     |  None |  65661.207231  | 10795.3380703 |\n",
      "|       lat        |  None | 704762.148408  | 1292011141.66 |\n",
      "|       long       |  None | -137780.01994  | 1292011141.57 |\n",
      "|  bed_bath_rooms  |  None | -8478.36410518 | 2858.95391257 |\n",
      "| bedrooms_squared |  None | -6072.38466067 | 1494.97042777 |\n",
      "| log_sqft_living  |  None | -563467.784269 | 17567.8230814 |\n",
      "|  lat_plus_long   |  None | -83217.1979248 | 1292011141.58 |\n",
      "+------------------+-------+----------------+---------------+\n",
      "[10 rows x 4 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine/extract each model's coefficients:\n",
    "print model_1.get(\"coefficients\")\n",
    "print model_2.get(\"coefficients\")\n",
    "print model_3.get(\"coefficients\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have the coefficents of our three models, we can compare what impacts the inclusion of new features has. We see a change in sign from the first model to the second, which tells us that when we add the interaction feature to the set, the bedroom coefficent goes the opposite direction. \\\\ \\\\\n",
    "Now that we have our two models, we can compute the RSS to find how well our models represent the real. We will first compute the RSS on our training data, which we can expect to be very similar, and the more complex model will have the lower training error, intuitively. Then we will run the RSS on the test set and see which model shows the best on data that it was not trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.71328233544e+14\n",
      "9.61592067856e+14\n",
      "9.05276314555e+14\n"
     ]
    }
   ],
   "source": [
    "# Compute the RSS on TRAINING data for each of the three models and record the values:\n",
    "RSS_1 = get_residual_sum_of_squares(model_1, train_data, train_data['price'])\n",
    "RSS_2 = get_residual_sum_of_squares(model_2, train_data, train_data['price'])\n",
    "RSS_3 = get_residual_sum_of_squares(model_3, train_data, train_data['price'])\n",
    "\n",
    "print RSS_1\n",
    "print RSS_2\n",
    "print RSS_3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Like expected, our RSS was lowest on the third model because it had the most features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.26568089093e+14\n",
      "2.24368799994e+14\n",
      "2.51829318952e+14\n"
     ]
    }
   ],
   "source": [
    "# Compute the RSS on TESTING data for each of the three models and record the values:\n",
    "RSS_1 = get_residual_sum_of_squares(model_1, test_data, test_data['price'])\n",
    "RSS_2 = get_residual_sum_of_squares(model_2, test_data, test_data['price'])\n",
    "RSS_3 = get_residual_sum_of_squares(model_3, test_data, test_data['price'])\n",
    "\n",
    "print RSS_1\n",
    "print RSS_2\n",
    "print RSS_3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we can see that the second model actually represented the best model with the lowest RSS on the test set. When we gave it more meaningful features, the predictive model improved and the test data fell closest to the predicted model. However, in the third model we added three more, less meaningful features (including one meaningless feature) and we see that it actually has the worst performing model. Feature selection is clearly a very important aspect of regression learning and will be covered in depth in a later section in regularization. \n",
    "\\\\ \\\\\n",
    "Now that we see how this polynomal regression behaves in GraphLab, lets implement some polynomial regression ourselves. Since we will be using numpy for our matrix calculations, we need to have a function that converts an SFrame into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted value 1181.0\n",
      "1181.0\n",
      "2571.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price')\n",
    "\n",
    "def get_numpy_data(data_sframe, features, output):\n",
    "    data_sframe['constant'] = 1 # this is how you add a constant column to an SFrame\n",
    "    # add the column 'constant' to the front of the features list so that we can extract it along with the others:\n",
    "    features = ['constant'] + features # this is how you combine two lists\n",
    "    # select the columns of data_SFrame given by the features list into the SFrame features_sframe (now including constant):\n",
    "    features_sframe = data_sframe[features]\n",
    "    # the following line will convert the features_SFrame into a numpy matrix:\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    # assign the column of data_sframe associated with the output to the SArray output_sarray\n",
    "    output_sarray = data_sframe[output]\n",
    "    # the following will convert the SArray into a numpy array by first converting it to a list\n",
    "    output_array = output_sarray.to_numpy()\n",
    "    return(feature_matrix, output_array)\n",
    "\n",
    "def predict_output(feature_matrix, weights):\n",
    "    return np.dot(feature_matrix, weights)\n",
    "\n",
    "my_weights = np.array([1., 1.]) # the example weights\n",
    "my_features = example_features[0,] # we'll use the first data point\n",
    "predicted_value = np.dot(my_features, my_weights)\n",
    "print 'predicted value ' + str(predicted_value)\n",
    "\n",
    "test_predictions = predict_output(example_features, my_weights)\n",
    "print test_predictions[0] # should be 1181.0\n",
    "print test_predictions[1] # should be 2571.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    # Assume that errors and feature are both numpy arrays of the same length (number of data points)\n",
    "    # compute twice the dot product of these vectors as 'derivative' and return the value\n",
    "    derivative = 2 * np.dot(errors, feature)\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23345850022.0\n",
      "-23345850022.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \n",
    "my_weights = np.array([0., 0.]) # this makes all the predictions 0\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "# just like SFrames 2 numpy arrays can be elementwise subtracted with '-': \n",
    "errors = test_predictions - example_output # prediction errors in this case is just the -example_output\n",
    "feature = example_features[:,0] # let's compute the derivative with respect to 'constant', the \":\" indicates \"all rows\"\n",
    "derivative = feature_derivative(errors, feature)\n",
    "print derivative\n",
    "print -np.sum(example_output)*2 # should be the same as derivative"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we've created all of our functions we need and computed our derivitives, its time to implement our gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt \n",
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    while not converged:\n",
    "        # compute the predictions based on feature_matrix and weights using your predict_output() function\n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "        # compute the errors as predictions - output\n",
    "        errors = predictions - output\n",
    "        gradient_sum_squares = 0 # initialize the gradient sum of squares\n",
    "        # while we haven't reached the tolerance yet, update each feature's weight\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n",
    "            # compute the derivative for weight[i]:\n",
    "            derivative = feature_derivative(errors, feature_matrix[:,i])\n",
    "            # add the squared value of the derivative to the gradient magnitude (for assessing convergence)\n",
    "            gradient_sum_squares = gradient_sum_squares + (derivative * derivative)\n",
    "            # subtract the step size times the derivative from the current weight\n",
    "            weights[i] = weights[i] - (step_size * derivative)\n",
    "        # compute the square-root of the gradient sum of squares to get the gradient matnigude:\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A few things to note before we run the gradient descent. Since the gradient is a sum over all the data points and involves a product of an error and a feature the gradient itself will be very large since the features are large and the output is large. So while you might expect tolerance to be small, small is only relative to the size of the features. \n",
    "\\\\ \\\\\n",
    "Although the gradient descent is designed for multiple regression since the constant is now a feature we can use the gradient descent function to estimat the parameters in the simple regression on squarefeet. The folowing cell sets up the feature\\_matrix, output, initial weights and step size for the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-46999.88716555    281.91211912]\n",
      "281.9\n"
     ]
    }
   ],
   "source": [
    "# let's test out the gradient descent\n",
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'\n",
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n",
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "tolerance = 2.5e7\n",
    "\n",
    "simple_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)\n",
    "print simple_weights\n",
    "print round(simple_weights[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
